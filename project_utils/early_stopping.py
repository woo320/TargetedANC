from collections import defaultdict

import numpy as np

class EarlyStoppingManager:
    """ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ ê´€ë¦¬ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""

    def __init__(self, patience, min_delta, metric_weights=None):
        self.patience = patience
        self.min_delta = min_delta
        
        # ğŸ”§ ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.metric_weights = metric_weights or {
            'anc_total': 0.4,           # ANC ì„±ëŠ¥ 40%
            'separation_loss': 0.3,     # ë¶„ë¦¬ ì„±ëŠ¥ 30%
            'classification_accuracy': 0.2,  # ë¶„ë¥˜ ì„±ëŠ¥ 20%
            'final_quality': 0.1        # ìµœì¢… í’ˆì§ˆ 10%
        }
        
        # ìƒíƒœ ê´€ë¦¬
        self.metrics_history = defaultdict(list)
        self.best_composite_score = -float('inf')
        self.best_scores = {}  # ê°œë³„ ë©”íŠ¸ë¦­ ë² ìŠ¤íŠ¸ ì ìˆ˜
        self.no_improvement_count = 0
        
        print(f"ğŸ”§ MultiMetric EarlyStopping initialized:")
        print(f"   Patience: {patience}, Min Delta: {min_delta}")
        print(f"   Weights: {self.metric_weights}")

    def compute_composite_score(self, metrics):
        """ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•œ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        score_details = {}
        
        # ANC ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ìŒìˆ˜ê°’ì´ë¯€ë¡œ ë¶€í˜¸ ì¡°ì •)
        anc_loss = metrics.get('anc_total', 0)
        anc_score = -anc_loss * self.metric_weights['anc_total']  # ìŒìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ
        score += anc_score
        score_details['anc_score'] = anc_score
        
        # ë¶„ë¦¬ ì†ì‹¤ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        sep_loss = metrics.get('separation_loss', 
                              (metrics.get('s1_separation', 0) + metrics.get('s2_separation', 0)) / 2)
        sep_score = -sep_loss * self.metric_weights['separation_loss']
        score += sep_score
        score_details['separation_score'] = sep_score
        
        # ë¶„ë¥˜ ì •í™•ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        cls_acc = metrics.get('classification_accuracy', 0)
        cls_score = cls_acc * self.metric_weights['classification_accuracy']
        score += cls_score
        score_details['classification_score'] = cls_score
        
        # ìµœì¢… í’ˆì§ˆ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        final_loss = metrics.get('final_quality', 0)
        final_score = -final_loss * self.metric_weights['final_quality']
        score += final_score
        score_details['final_quality_score'] = final_score
        
        return score, score_details

    def update(self, metrics):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ íˆìŠ¤í† ë¦¬ ì €ì¥
        for key, value in metrics.items():
            self.metrics_history[key].append(value)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        current_score, score_details = self.compute_composite_score(metrics)
        
        # ê°œë³„ ë©”íŠ¸ë¦­ ë² ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë¡œê¹…ìš©)
        for metric_name, value in metrics.items():
            if metric_name not in self.best_scores:
                self.best_scores[metric_name] = value
            elif metric_name in ['anc_total', 'separation_loss', 'final_quality']:
                # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                if value < self.best_scores[metric_name]:
                    self.best_scores[metric_name] = value
            elif metric_name == 'classification_accuracy':
                # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                if value > self.best_scores[metric_name]:
                    self.best_scores[metric_name] = value
        
        # ì¢…í•© ì ìˆ˜ ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ íŒë‹¨
        improvement = current_score - self.best_composite_score
        
        if improvement > self.min_delta:
            self.best_composite_score = current_score
            self.no_improvement_count = 0
            
            # ìƒì„¸ ì •ë³´ ì¶œë ¥ (ê°œì„  ì‹œì—ë§Œ)
            print(f"ğŸ¯ Early Stopping: NEW BEST composite score: {current_score:.4f}")
            print(f"   ğŸ“Š Score breakdown: ANC={score_details['anc_score']:.3f}, "
                  f"Sep={score_details['separation_score']:.3f}, "
                  f"Cls={score_details['classification_score']:.3f}, "
                  f"Qual={score_details['final_quality_score']:.3f}")
            
        else:
            self.no_improvement_count += 1
            
            # ê°œì„  ì—†ìŒ ê²½ê³  (ê°€ë”ì”©ë§Œ)
            if self.no_improvement_count % 2 == 0:
                print(f"â³ Early Stopping: No improvement for {self.no_improvement_count}/{self.patience} epochs")

    def should_stop(self):
        """ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€ ê²°ì •"""
        return self.no_improvement_count >= self.patience
    
    def get_best_scores(self):
        """ë² ìŠ¤íŠ¸ ì ìˆ˜ë“¤ ë°˜í™˜ (í˜¸í™˜ì„±)"""
        return {
            'composite_score': self.best_composite_score,
            'individual_bests': self.best_scores.copy()
        }
    
    def get_improvement_summary(self):
        """ê°œì„  ìƒí™© ìš”ì•½"""
        if not self.metrics_history:
            return "No metrics recorded yet"
        
        summary = []
        for metric_name in ['anc_total', 'separation_loss', 'classification_accuracy', 'final_quality']:
            if metric_name in self.metrics_history and len(self.metrics_history[metric_name]) >= 2:
                values = self.metrics_history[metric_name]
                initial = values[0]
                current = values[-1]
                
                if metric_name == 'classification_accuracy':
                    change = current - initial
                    direction = "â†‘" if change > 0 else "â†“"
                    summary.append(f"{metric_name}: {initial:.3f} â†’ {current:.3f} {direction}")
                else:
                    change = initial - current  # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                    direction = "â†“" if change > 0 else "â†‘"  
                    summary.append(f"{metric_name}: {initial:.3f} â†’ {current:.3f} {direction}")
        
        return " | ".join(summary)
