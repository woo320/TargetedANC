from collections import defaultdict
import numpy as np

class EarlyStoppingManager:
    """
    [ì—­í• ]
    ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê¸°ë°˜ Early Stopping Manager
    ì—¬ëŸ¬ ì„±ëŠ¥ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€ë¥¼ ê²°ì •
    """

    def __init__(self, patience, min_delta, metric_weights=None):

        self.patience = patience
        self.min_delta = min_delta
        
        # ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’ ì„¤ì •
        self.metric_weights = metric_weights or {
            'anc_total': 0.4,                
            'separation_loss': 0.3,          
            'classification_accuracy': 0.2,  
            'final_quality': 0.1             
        }
        
        self.metrics_history = defaultdict(list)   
        self.best_composite_score = -float('inf')  
        self.best_scores = {}                      
        self.no_improvement_count = 0              
        
        print(f"ğŸ”§ ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ì¡°ê¸° ì¢…ë£Œ ê´€ë¦¬ì ì´ˆê¸°í™”:")
        print(f"   ì¸ë‚´ì‹¬: {patience}, ìµœì†Œ ë³€í™”ëŸ‰: {min_delta}")
        print(f"   ê°€ì¤‘ì¹˜: {self.metric_weights}")

    def compute_composite_score(self, metrics):
        """
        [ì—­í• ] ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ì¡°í•©í•´ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        """
        
        score = 0.0
        score_details = {}
        
        # 1. ANC ì ìˆ˜
        anc_loss = metrics.get('anc_total', 0)
        anc_score = -anc_loss * self.metric_weights['anc_total']
        score += anc_score
        score_details['anc_score'] = anc_score
        
        # 2. ë¶„ë¦¬ ì ìˆ˜
        sep_loss = metrics.get('separation_loss', 
                              (metrics.get('s1_separation', 0) + metrics.get('s2_separation', 0)) / 2)
        sep_score = -sep_loss * self.metric_weights['separation_loss']
        score += sep_score
        score_details['separation_score'] = sep_score
        
        # 3. ë¶„ë¥˜ ì ìˆ˜
        cls_acc = metrics.get('classification_accuracy', 0)
        cls_score = cls_acc * self.metric_weights['classification_accuracy']
        score += cls_score
        score_details['classification_score'] = cls_score
        
        # 4. ìµœì¢… ì ìˆ˜
        final_loss = metrics.get('final_quality', 0)
        final_score = -final_loss * self.metric_weights['final_quality']
        score += final_score
        score_details['final_quality_score'] = final_score
        
        return score, score_details

    def update(self, metrics):
        """
        [ì—­í• ]
        ìƒˆë¡œìš´ ì—í¬í¬ì˜ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        ì¢…í•© ì ìˆ˜ ê¸°ë°˜ ê°œì„  íŒë‹¨
        """

        for key, value in metrics.items():
            self.metrics_history[key].append(value)
        
        current_score, score_details = self.compute_composite_score(metrics)
        
        for metric_name, value in metrics.items():
            if metric_name not in self.best_scores:
                self.best_scores[metric_name] = value

            elif metric_name in ['anc_total', 'separation_loss', 'final_quality']:
                if value < self.best_scores[metric_name]:
                    self.best_scores[metric_name] = value

            elif metric_name == 'classification_accuracy':
                if value > self.best_scores[metric_name]:
                    self.best_scores[metric_name] = value
        
        improvement = current_score - self.best_composite_score
        
        if improvement > self.min_delta:
            self.best_composite_score = current_score
            self.no_improvement_count = 0
            
            print(f"  ì¡°ê¸° ì¢…ë£Œ: ìƒˆë¡œìš´ ìµœê³  ì¢…í•© ì ìˆ˜: {current_score:.4f}")
            print(f"  ì ìˆ˜ ì„¸ë¶€ì‚¬í•­: ANC={score_details['anc_score']:.3f}, "
                  f"ë¶„ë¦¬={score_details['separation_score']:.3f}, "
                  f"ë¶„ë¥˜={score_details['classification_score']:.3f}, "
                  f"í’ˆì§ˆ={score_details['final_quality_score']:.3f}")
            
        else:
            self.no_improvement_count += 1
            
            if self.no_improvement_count % 2 == 0:
                print(f"  ì¡°ê¸° ì¢…ë£Œ: {self.no_improvement_count}/{self.patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")

    def should_stop(self):
        """
        [ì—­í• ] ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€ ê²°ì •
        """
        return self.no_improvement_count >= self.patience
    
    def get_best_scores(self):
        """
        [ì—­í• ] ì¢…í•© ì ìˆ˜ì™€ ê°œë³„ ìµœê³  ì ìˆ˜ ë°˜í™˜ (í˜¸í™˜ì„± ìœ ì§€)
        """
        return {
            'composite_score': self.best_composite_score,
            'individual_bests': self.best_scores.copy()
        }
    
    def get_improvement_summary(self):
        """
        [ì—­í• ]
        í•™ìŠµ ê³¼ì •ì—ì„œì˜ ê°œì„  ìƒí™© ìš”ì•½
        ê° ë©”íŠ¸ë¦­ ì´ˆê¸°ê°’ vs í˜„ì¬ê°’ ë¹„êµ í›„ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ í‘œì‹œ
        """

        if not self.metrics_history:
            return "ì•„ì§ ê¸°ë¡ëœ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤"
        
        summary = []
        
        for metric_name in ['anc_total', 'separation_loss', 'classification_accuracy', 'final_quality']:
            if metric_name in self.metrics_history and len(self.metrics_history[metric_name]) >= 2:
                values = self.metrics_history[metric_name]
                initial = values[0]    
                current = values[-1]
                
                if metric_name == 'classification_accuracy':
                    # ì •í™•ë„
                    change = current - initial
                    direction = "â†‘" if change > 0 else "â†“"
                    summary.append(f"{metric_name}: {initial:.3f} â†’ {current:.3f} {direction}")
                else:
                    # ì†ì‹¤
                    change = initial - current
                    direction = "â†“" if change > 0 else "â†‘"  
                    summary.append(f"{metric_name}: {initial:.3f} â†’ {current:.3f} {direction}")
        
        return " | ".join(summary)