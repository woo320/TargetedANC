"""
ì •ë¦¬ëœ Joint Model (ë””ë²„ê·¸ ì½”ë“œ ìµœì†Œí™”)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import sys
import numpy as np
from scipy.io import loadmat

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.insert(0, "/content/drive/MyDrive/joint/sudo_rm_rf")
sys.path.insert(0, "/content/drive/MyDrive/joint/WaveNet-VNNs-for-ANC/WaveNet_VNNs")

# SudoRM-RF ê´€ë ¨ import
try:
    from sudo_rm_rf.dnn.models.causal_improved_sudormrf_v3 import CausalSuDORMRF
    import sudo_rm_rf.dnn.experiments.utils.mixture_consistency as mixture_consistency
except ImportError:
    try:
        from sudo_rm_rf.dnn.models.causal_improved_sudormrf_v3 import CausalSuDORMRF
        import sudo_rm_rf.dnn.utils.mixture_consistency as mixture_consistency
    except ImportError:
        print("âŒ SudoRM-RF import failed")
        raise

# WaveNet ê´€ë ¨ import
from networks import WaveNet_VNNs
from utils import fir_filter, SEF

# ë¡œì»¬ ëª¨ë“ˆ import
from project_utils.audio_utils import standardize_audio_dims
from config.constants import SR

# BroadcastClassifier import
try:
    from models.broadcast_classifier import BroadcastClassifier
    from models.model_utils import load_broadcast_classifier_weights
except ImportError:
    try:
        from .broadcast_classifier import BroadcastClassifier
        from .model_utils import load_broadcast_classifier_weights
    except ImportError:
        BroadcastClassifier = None

class ImprovedJointModel(nn.Module):
    """ì •ë¦¬ëœ ì¡°ì¸íŠ¸ ëª¨ë¸"""

    def __init__(self, sudormrf_checkpoint_path, wavenet_checkpoint_path, wavenet_config_path,
                 broadcast_classifier_checkpoint_path=None, use_broadcast_classifier=False, 
                 model_config=None):
        super().__init__()

        self.model_config = model_config or {}
        self.eta_init_value = self.model_config.get('eta_init_value', 0.1)
        self.sef_clamp_range = self.model_config.get('sef_clamp_range', (-10.0, 10.0))
        self.classification_window_len = self.model_config.get('classification_window_len', 16000)
        self.audio_eps = self.model_config.get('audio_eps', 1e-9)

        from trainers.memory_manager import MemoryManager
        self.memory_manager = MemoryManager(self.model_config)
        self.use_broadcast_classifier = use_broadcast_classifier

        # SudoRM-RF ëª¨ë¸ ë¡œë“œ
        sudormrf_config = {
            'in_audio_channels': 1, 'out_channels': 256, 'in_channels': 384,
            'num_blocks': 16, 'upsampling_depth': 5, 'enc_kernel_size': 21,
            'enc_num_basis': 256, 'num_sources': 2
        }

        self.separation_model = CausalSuDORMRF(**sudormrf_config)
        self._load_sudormrf_weights(sudormrf_checkpoint_path)

        # WaveNet ëª¨ë¸ ë¡œë“œ
        with open(wavenet_config_path, 'r') as f:
            wavenet_config = json.load(f)

        self.noise_reduction_model = WaveNet_VNNs(wavenet_config)
        self._load_wavenet_weights(wavenet_checkpoint_path)

        # BroadcastClassifier ì¶”ê°€
        if self.use_broadcast_classifier and BroadcastClassifier is not None:
            self.broadcast_classifier = BroadcastClassifier(window_len=16000)
            if broadcast_classifier_checkpoint_path and load_broadcast_classifier_weights:
                self.broadcast_classifier = load_broadcast_classifier_weights(
                    self.broadcast_classifier, broadcast_classifier_checkpoint_path
                )
            print("âœ… BroadcastClassifier added")
        else:
            self.broadcast_classifier = None

        # ANC íŒŒë¼ë¯¸í„° ì„¤ì •
        self._setup_anc_paths()

        print("âœ… Joint model loaded")

    def _setup_anc_paths(self):
        """ANC path ì„¤ì •"""
        try:
            pri_path = "/content/drive/MyDrive/joint/WaveNet-VNNs-for-ANC/WaveNet_VNNs/pri_channel.mat"
            sec_path = "/content/drive/MyDrive/joint/WaveNet-VNNs-for-ANC/WaveNet_VNNs/sec_channel.mat"
            
            pri_channel = torch.tensor(
                loadmat(pri_path)["pri_channel"].squeeze(), 
                dtype=torch.float
            )
            sec_channel = torch.tensor(
                loadmat(sec_path)["sec_channel"].squeeze(), 
                dtype=torch.float
            )
            
            self.square_eta = self.model_config.get('eta_init_value', 0.1)
            
            # register_bufferë¡œ ë“±ë¡
            self.register_buffer('pri_channel', pri_channel)
            self.register_buffer('sec_channel', sec_channel)
            self.register_buffer('pri_filter', pri_channel)
            self.register_buffer('sec_filter', sec_channel)
            self.eta = nn.Parameter(torch.tensor(self.square_eta))
            
        except Exception as e:
            print(f"âš ï¸ Failed to load ANC paths: {e}")
            # Fallback: ë”ë¯¸ í•„í„° ìƒì„±
            filter_len = 64
            pri_channel = torch.randn(filter_len) * 0.01
            sec_channel = torch.randn(filter_len) * 0.01
            self.square_eta = self.model_config.get('eta_init_value', 0.1)
            
            self.register_buffer('pri_channel', pri_channel)
            self.register_buffer('sec_channel', sec_channel)
            self.register_buffer('pri_filter', pri_channel)
            self.register_buffer('sec_filter', sec_channel)
            self.eta = nn.Parameter(torch.tensor(self.square_eta))

    def _load_sudormrf_weights(self, checkpoint_path):
        """SudoRM-RF ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        state = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

        if 'model_state_dict' in state:
            actual_state = state['model_state_dict']
        elif 'model' in state:
            actual_state = state['model']
        else:
            actual_state = state

        # module. ì ‘ë‘ì‚¬ ì œê±°
        if any(key.startswith('module.') for key in actual_state.keys()):
            new_state = {key[7:] if key.startswith('module.') else key: value
                        for key, value in actual_state.items()}
            actual_state = new_state

        self.separation_model.load_state_dict(actual_state)

    def _load_wavenet_weights(self, checkpoint_path):
        """WaveNet ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        state = torch.load(checkpoint_path, map_location='cpu')
        model_state = state['model'] if 'model' in state else state
        self.noise_reduction_model.load_state_dict(model_state)

    def _sef_nonlinearity(self, signal, eta):        
        clamp_min, clamp_max = self.sef_clamp_range
        signal = torch.clamp(signal, clamp_min, clamp_max)

        # ì…ë ¥ ê²€ì¦
        if torch.isnan(signal).any() or torch.isinf(signal).any():
            signal = torch.nan_to_num(signal, nan=0.0, posinf=1.0, neginf=-1.0)
        
        clamp_min, clamp_max = self.sef_clamp_range

        # SEF í•¨ìˆ˜ í˜¸ì¶œ ì „ ì°¨ì› í™•ì¸
        if signal.dim() == 3 and signal.shape[1] == 1:
            signal_for_sef = signal.squeeze(1)
        elif signal.dim() == 2:
            signal_for_sef = signal
        else:
            signal_for_sef = signal

        try:
            result = SEF(signal_for_sef, eta)
            
            if torch.isnan(result).any() or torch.isinf(result).any():
                result = torch.nan_to_num(result, nan=0.0, posinf=1.0, neginf=-1.0)
            
            if result.dim() == 2:
                result = result.unsqueeze(1)
            
            return result
            
        except Exception:
            # Fallback: ì›ë³¸ ì‹ í˜¸ ë°˜í™˜
            if signal_for_sef.dim() == 2:
                return signal_for_sef.unsqueeze(1)
            else:
                return signal_for_sef

    def preprocess_sep_chunk(self, wav_chunk, chunk_len):
        """ì¶”ë¡ ì½”ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬"""
        T = wav_chunk.shape[-1]
        if T < chunk_len:
            wav_chunk = F.pad(wav_chunk, (0, chunk_len - T), mode='constant')
        else:
            wav_chunk = wav_chunk[..., :chunk_len]

        m_mean = wav_chunk.mean(dim=-1, keepdim=True)
        m_std = wav_chunk.std(dim=-1, keepdim=True)
        normalized = (wav_chunk - m_mean) / (m_std + 1e-9)
        return normalized, m_mean, m_std, min(T, chunk_len)

    def postprocess_sep_chunk(self, est, m_mean, m_std, valid_len):
        """ì¶”ë¡ ì½”ë“œì™€ ë™ì¼í•œ í›„ì²˜ë¦¬"""
        out = (est * (m_std + 1e-9) + m_mean)
        return out[..., :valid_len]

    def compute_sigmoid_mask(self, signal, classifier, device):
        window_len = self.classification_window_len
        L = signal.shape[-1]
        """ì¶”ë¡ ì½”ë“œì™€ ë™ì¼í•œ sigmoid mask ê³„ì‚°"""
        if classifier is None:
            return torch.zeros(signal.shape[-1], dtype=torch.float32, device=device)
            
        L = signal.shape[-1]
        masks = torch.zeros(L, dtype=torch.float32, device=device)

        sig_np = signal.squeeze().detach().cpu().numpy()

        for start in range(0, L, window_len):
            end = min(start + window_len, L)
            seg = sig_np[start:end]
            if seg.shape[0] < window_len:
                seg = np.pad(seg, (0, window_len - seg.shape[0]), mode="constant")

            x = torch.from_numpy(seg.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)
            with torch.no_grad():
                logit = classifier(x)
                prob_chunk = torch.sigmoid(logit).item()
            masks[start:end] = prob_chunk

        return masks

    def reduce_noise_like_inference(self, noise_signal, device):
        """ì¶”ë¡ ì½”ë“œì™€ ë™ì¼í•œ ANC ì²˜ë¦¬"""
        SEG_LEN = 10 * SR  # 10ì´ˆ ì„¸ê·¸ë¨¼íŠ¸
        N = noise_signal.shape[-1]
        outs_en, outs_dn = [], []

        with torch.no_grad():
            for start in range(0, N, SEG_LEN):
                end = min(start + SEG_LEN, N)
                seg = noise_signal[..., start:end]
                T = seg.shape[-1]

                tgt_gpu = fir_filter(self.pri_filter.to(device), seg)
                
                out = self.noise_reduction_model(seg)
                if out.dim() == 3:
                    out_flat = out.squeeze(1)
                elif out.dim() == 2:
                    out_flat = out
                else:
                    out_flat = out.unsqueeze(0)

                nl = SEF(out_flat, self.eta)
                dn = fir_filter(self.sec_filter.to(device), nl)
                en = tgt_gpu + dn

                outs_en.append(en[..., :T])
                outs_dn.append(dn[..., :T])

        enhanced = torch.cat(outs_en, dim=-1)
        anti_noise = torch.cat(outs_dn, dim=-1)
        return enhanced, anti_noise

    def _forward_direct_safe(self, mixed_input, chunk_len=None, return_classification=False):
        """gradient-safe forward - ë””ë²„ê·¸ ì½”ë“œ ì •ë¦¬ ë²„ì „"""
        device = mixed_input.device
        
        # ì…ë ¥ ì°¨ì› ê²€ì¦ ë° ìˆ˜ì •
        if mixed_input.dim() == 4:
            mixed_input = mixed_input.squeeze(2)
        elif mixed_input.dim() == 2:
            mixed_input = mixed_input.unsqueeze(1)
        
        # ì „ì²´ ê¸¸ì´ì™€ ì²­í¬ ê¸¸ì´ í™•ì¸
        total_length = mixed_input.shape[-1]
        
        if chunk_len is None:
            chunk_len = self.memory_manager.get_safe_chunk_size()
        
        # ì²­í¬ ê¸¸ì´ê°€ ì „ì²´ ê¸¸ì´ë³´ë‹¤ í´ ê²½ìš° ì¡°ì •
        if chunk_len > total_length:
            chunk_len = total_length
        
        # Separation model ì²˜ë¦¬
        with torch.amp.autocast('cuda', enabled=False):
            if chunk_len < total_length:
                # ì²­í¬ë³„ ë¶„ë¦¬ ì²˜ë¦¬
                sep_chunks = []
                for start in range(0, total_length, chunk_len):
                    end = min(start + chunk_len, total_length)
                    chunk = mixed_input[:, :, start:end]
                    chunk_output = self.separation_model(chunk)
                    sep_chunks.append(chunk_output)
                
                sep_output = torch.cat(sep_chunks, dim=-1)
            else:
                sep_output = self.separation_model(mixed_input)
        
        # 2ì±„ë„ë¡œ ë¶„ë¦¬
        s1_clean = sep_output[:, 0:1, :]  # ë°©ì†¡ ì±„ë„
        s2_noise = sep_output[:, 1:2, :]  # ë…¸ì´ì¦ˆ ì±„ë„

        # Classification and Channel Assignment
        classification_results = None
        broadcast_channel = s1_clean  # ê¸°ë³¸ê°’
        noise_channel = s2_noise      # ê¸°ë³¸ê°’

        if return_classification and self.broadcast_classifier is not None:
            try:
                with torch.amp.autocast('cuda', enabled=True):
                    # 1. ë‘ ì±„ë„ ëª¨ë‘ ì „ì²˜ë¦¬
                    ch0_for_classification = self._prepare_for_classification(s1_clean)
                    ch1_for_classification = self._prepare_for_classification(s2_noise)
                    
                    # 2. ê°ê° ë¶„ë¥˜ (ì›ë˜ í•™ìŠµëœ ë°©ì‹ ê·¸ëŒ€ë¡œ)
                    ch0_logits = self.broadcast_classifier(ch0_for_classification)  # [1, 1]
                    ch1_logits = self.broadcast_classifier(ch1_for_classification)  # [1, 1]
                    
                    # 3. í™•ë¥  ë³€í™˜
                    ch0_prob = torch.sigmoid(ch0_logits).mean().item()  # 0~1 ì‚¬ì´ í™•ë¥ 
                    ch1_prob = torch.sigmoid(ch1_logits).mean().item()  # 0~1 ì‚¬ì´ í™•ë¥ 
                    
                    print(f"ğŸ“Š Ch0 (s1) broadcast prob: {ch0_prob:.3f}")
                    print(f"ğŸ“Š Ch1 (s2) broadcast prob: {ch1_prob:.3f}")
                    
                    # 4. ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ìª½ì´ ë°©ì†¡
                    if ch0_prob > ch1_prob:
                        print("âœ… Ch0 â†’ Broadcast, Ch1 â†’ Noise")
                        broadcast_channel = s1_clean  # ch0 = ë°©ì†¡
                        noise_channel = s2_noise      # ch1 = ë…¸ì´ì¦ˆ
                        is_ch0_broadcast = True
                    else:
                        print("ğŸ”„ Ch0 â†’ Noise, Ch1 â†’ Broadcast")
                        broadcast_channel = s2_noise  # ch1 = ë°©ì†¡  
                        noise_channel = s1_clean      # ch0 = ë…¸ì´ì¦ˆ
                        is_ch0_broadcast = False
                    
                    # 5. ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
                    classification_results = {
                        'ch0_prob': ch0_prob,
                        'ch1_prob': ch1_prob,
                        'ch0_logits': ch0_logits,
                        'ch1_logits': ch1_logits,
                        'is_ch0_broadcast': is_ch0_broadcast,
                        'confidence': abs(ch0_prob - ch1_prob)
                    }
                    
            except Exception as e:
                print(f"âš ï¸ Classification failed: {e}")
                # í´ë°±: ê¸°ë³¸ í• ë‹¹
                broadcast_channel = s1_clean
                noise_channel = s2_noise
                classification_results = None

        # WaveNet ìŠ¤íƒ€ì¼ ANC ì²˜ë¦¬
        with torch.amp.autocast('cuda', enabled=True):
            noise_input = noise_channel.squeeze(1)  # [B, T]
            
            # deviceì— ë§ê²Œ ì±„ë„ ì´ë™
            pri_channel = self.pri_channel.to(device)
            sec_channel = self.sec_channel.to(device)
            
            # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì‘ì€ ì²­í¬ í¬ê¸°
            anc_chunk_len = min(16000, total_length // 4)
            
            if anc_chunk_len < total_length and total_length > 16000:
                # ì²­í¬ë³„ ANC ì²˜ë¦¬
                chunks_target = []
                chunks_dn = []
                chunks_en = []
                
                for i in range(0, total_length, anc_chunk_len):
                    end_idx = min(i + anc_chunk_len, total_length)
                    actual_chunk_len = end_idx - i
                    
                    chunk = noise_input[:, i:end_idx]  # [B, actual_chunk_len]
                    
                    if chunk.shape[-1] > 0:
                        # ì°¨ì› ë³€í™˜: [B, T] -> [B, 1, T]
                        chunk_unsqueezed = chunk.unsqueeze(1)
                        
                        # FIR í•„í„° ì ìš©
                        target_chunk = fir_filter(pri_channel, chunk_unsqueezed)
                        
                        # WaveNet ëª¨ë¸ í˜¸ì¶œ
                        wavenet_output = self.noise_reduction_model(chunk_unsqueezed)

                        if torch.isnan(wavenet_output).any() or torch.isinf(wavenet_output).any():
                            wavenet_output = torch.nan_to_num(wavenet_output, nan=0.0, posinf=1.0, neginf=-1.0)
                        
                        # WaveNet ì¶œë ¥ ì°¨ì› ì •ê·œí™”
                        if wavenet_output.dim() == 3 and wavenet_output.shape[1] == 1:
                            wavenet_flat = wavenet_output.squeeze(1)
                        elif wavenet_output.dim() == 2:
                            wavenet_flat = wavenet_output
                        elif wavenet_output.dim() == 1:
                            wavenet_flat = wavenet_output.unsqueeze(0)
                        else:
                            wavenet_flat = wavenet_output.view(chunk.shape[0], -1)
                        
                        # SEF ë¹„ì„ í˜• ì²˜ë¦¬
                        nonlinear_chunk = self._sef_nonlinearity(wavenet_flat, self.square_eta)
                        
                        # fir_filterë¥¼ ìœ„í•´ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                        if nonlinear_chunk.dim() == 2:
                            nonlinear_for_filter = nonlinear_chunk.unsqueeze(1)
                        elif nonlinear_chunk.dim() == 3:
                            nonlinear_for_filter = nonlinear_chunk
                        else:
                            nonlinear_for_filter = nonlinear_chunk.view(chunk.shape[0], 1, -1)
                        
                        # FIR í•„í„° ì ìš©
                        dn_chunk = fir_filter(sec_channel, nonlinear_for_filter)
                        
                        # WaveNet ìŠ¤íƒ€ì¼ í•©ì„±
                        en_chunk = dn_chunk + target_chunk
                        
                        # í¬ê¸° ê²€ì¦ ë° ìˆ˜ì • í•¨ìˆ˜
                        def fix_chunk_shape(tensor, expected_shape):
                            if tensor.shape != expected_shape:
                                # ë°°ì¹˜ ì°¨ì› ìˆ˜ì •
                                if tensor.shape[0] != expected_shape[0]:
                                    if tensor.shape[0] == expected_shape[2]:
                                        tensor = tensor.permute(2, 1, 0) if tensor.dim() == 3 else tensor.permute(1, 0)
                                        if tensor.dim() == 2:
                                            tensor = tensor.unsqueeze(1)
                                    else:
                                        tensor = tensor.view(expected_shape)
                                
                                # ì±„ë„ ì°¨ì› ìˆ˜ì •
                                if tensor.dim() == 2:
                                    tensor = tensor.unsqueeze(1)
                                elif tensor.dim() == 3 and tensor.shape[1] != 1:
                                    tensor = tensor[:, :1, :]
                                
                                # ê¸¸ì´ ì°¨ì› ìˆ˜ì •
                                if tensor.shape[-1] != expected_shape[-1]:
                                    tensor = tensor[..., :expected_shape[-1]]
                                
                                # ìµœì¢… í¬ê¸° í™•ì¸
                                if tensor.shape != expected_shape:
                                    tensor = tensor.view(expected_shape)
                            
                            return tensor
                        
                        expected_shape = (chunk.shape[0], 1, actual_chunk_len)
                        target_chunk = fix_chunk_shape(target_chunk, expected_shape)
                        dn_chunk = fix_chunk_shape(dn_chunk, expected_shape)
                        en_chunk = fix_chunk_shape(en_chunk, expected_shape)
                        
                        chunks_target.append(target_chunk)
                        chunks_dn.append(dn_chunk)
                        chunks_en.append(en_chunk)
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        del chunk_unsqueezed, target_chunk, wavenet_output, wavenet_flat
                        del nonlinear_chunk, nonlinear_for_filter, dn_chunk, en_chunk
                        torch.cuda.empty_cache()
                
                if chunks_target:
                    try:
                        # ë©”ëª¨ë¦¬ ì ˆì•½ ì—°ê²°
                        with torch.cuda.device(device):
                            torch.cuda.empty_cache()
                            
                            s2_target = torch.cat(chunks_target, dim=-1)
                            del chunks_target
                            torch.cuda.empty_cache()
                            
                            s2_antinoise = torch.cat(chunks_dn, dim=-1)
                            del chunks_dn
                            torch.cuda.empty_cache()
                            
                            s2_enhanced = torch.cat(chunks_en, dim=-1)
                            del chunks_en
                            torch.cuda.empty_cache()
                        
                    except Exception as e:
                        # í´ë°±: ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì‚¬ìš©
                        first_target = chunks_target[0] if chunks_target else torch.zeros(1, 1, anc_chunk_len, device=device)
                        first_dn = chunks_dn[0] if chunks_dn else torch.zeros(1, 1, anc_chunk_len, device=device)
                        first_en = chunks_en[0] if chunks_en else torch.zeros(1, 1, anc_chunk_len, device=device)
                        
                        remaining_len = total_length - first_target.shape[-1]
                        if remaining_len > 0:
                            zero_pad = torch.zeros(1, 1, remaining_len, device=device)
                            s2_target = torch.cat([first_target, zero_pad], dim=-1)
                            s2_antinoise = torch.cat([first_dn, zero_pad], dim=-1)
                            s2_enhanced = torch.cat([first_en, zero_pad], dim=-1)
                        else:
                            s2_target = first_target
                            s2_antinoise = first_dn
                            s2_enhanced = first_en
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        del chunks_target, chunks_dn, chunks_en
                        torch.cuda.empty_cache()
                else:
                    s2_target = torch.zeros_like(s2_noise)
                    s2_antinoise = torch.zeros_like(s2_noise)
                    s2_enhanced = torch.zeros_like(s2_noise)
            else:
                # ì „ì²´ ê¸¸ì´ ANC ì²˜ë¦¬
                try:
                    noise_unsqueezed = noise_input.unsqueeze(1)
                    
                    s2_target = fir_filter(pri_channel, noise_unsqueezed)
                    
                    # WaveNet ëª¨ë¸
                    wavenet_output = self.noise_reduction_model(noise_unsqueezed)
                    
                    # WaveNet ì¶œë ¥ ì°¨ì› ì¡°ì •
                    if wavenet_output.dim() == 3:
                        wavenet_flat = wavenet_output.squeeze(1)
                    elif wavenet_output.dim() == 2:
                        wavenet_flat = wavenet_output
                    else:
                        wavenet_flat = wavenet_output.view(wavenet_output.shape[0], -1)
                    
                    # SEF ë¹„ì„ í˜•
                    nonlinear_out = self._sef_nonlinearity(wavenet_flat, self.square_eta)
                    
                    # fir_filterìš© ì°¨ì› í™•ì¸
                    if nonlinear_out.dim() == 2:
                        nonlinear_for_filter = nonlinear_out.unsqueeze(1)
                    elif nonlinear_out.dim() == 3:
                        nonlinear_for_filter = nonlinear_out
                    else:
                        nonlinear_for_filter = nonlinear_out
                    
                    s2_antinoise = fir_filter(sec_channel, nonlinear_for_filter)
                    
                    # en = dn + target
                    s2_enhanced = s2_antinoise + s2_target
                    
                except Exception:
                    s2_target = torch.zeros_like(s2_noise)
                    s2_antinoise = torch.zeros_like(s2_noise)
                    s2_enhanced = torch.zeros_like(s2_noise)
        
        # ìµœì¢… ì¬í•©ì„±
        if hasattr(self, 'eta'):
            eta_weight = torch.sigmoid(self.eta)
            enhanced_verification = eta_weight * broadcast_channel + (1 - eta_weight) * s2_enhanced
        else:
            enhanced_verification = 0.7 * broadcast_channel + 0.3 * s2_enhanced
            
        return {
            's1_clean': broadcast_channel,
            's2_noise': noise_channel,
            's2_target': s2_target,
            's2_antinoise': s2_antinoise,
            's2_enhanced': s2_enhanced,
            'enhanced_verification': enhanced_verification,
            'classification': classification_results
        }

    def forward_inference_style(self, mixed_input, chunk_len=None, return_classification=False):
        batch_size = mixed_input.shape[0]
        L = mixed_input.shape[-1]
        device = mixed_input.device

        # ì ì‘ì  ì²­í¬ í¬ê¸° ê²°ì •
        if chunk_len is None:
            chunk_len = self.memory_manager.get_safe_chunk_size()

        # ì…ë ¥ ì°¨ì› í‘œì¤€í™”
        mixed_input = standardize_audio_dims(mixed_input)

        # === 1ë‹¨ê³„: ë¶„ë¦¬(Separation) ===
        max_num_sources = 2
        sep_acc = torch.zeros((batch_size, max_num_sources, L), dtype=torch.float32, device=device)

        for start in range(0, L, chunk_len):
            end = min(start + chunk_len, L)
            chunk = mixed_input[..., start:end]
            
            # ì „ì²˜ë¦¬ (ë°°ì¹˜ ë‹¨ìœ„)
            mix_t, m_mean, m_std, valid = self.preprocess_sep_chunk(chunk, chunk_len)
            
            # ë¶„ë¦¬
            with torch.no_grad():
                est = self.separation_model(mix_t)
                est = mixture_consistency.apply(est, mix_t)
            
            # í›„ì²˜ë¦¬ (ë°°ì¹˜ ë‹¨ìœ„)
            sep_chunk = self.postprocess_sep_chunk(est, m_mean, m_std, valid)
            sep_acc[:, :, start:start+valid] = sep_chunk

        # === 2ë‹¨ê³„: ë¶„ë¥˜ ë° ì±„ë„ í• ë‹¹ ===
        classification_results = {}
        
        if self.use_broadcast_classifier and self.broadcast_classifier is not None:
            # ğŸ”§ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„ë¥˜ ìˆ˜í–‰ (ìˆ˜ì •ëœ ë°©ì‹)
            batch_broadcast_channels = []
            batch_noise_channels = []
            batch_classification_info = []
            
            for b in range(batch_size):
                # ê° ìƒ˜í”Œì˜ ë‘ ì±„ë„ì„ _prepare_for_classification ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜
                ch0_tensor = sep_acc[b, 0:1].unsqueeze(0)  # [1, 1, T]
                ch1_tensor = sep_acc[b, 1:2].unsqueeze(0)  # [1, 1, T]
                
                # ë¶„ë¥˜ê¸°ìš© ì „ì²˜ë¦¬
                ch0_prep = self._prepare_for_classification(ch0_tensor)  # [1, 1, 16000]
                ch1_prep = self._prepare_for_classification(ch1_tensor)  # [1, 1, 16000]
                
                # ê° ì±„ë„ì˜ ë°©ì†¡ í™•ë¥  ê³„ì‚°
                with torch.no_grad():
                    ch0_logits = self.broadcast_classifier(ch0_prep)
                    ch1_logits = self.broadcast_classifier(ch1_prep)
                    
                    ch0_prob = torch.sigmoid(ch0_logits).mean().item()
                    ch1_prob = torch.sigmoid(ch1_logits).mean().item()
                
                print(f"ğŸ“Š Sample {b}: Ch0 prob={ch0_prob:.3f}, Ch1 prob={ch1_prob:.3f}")
                
                # ğŸ¯ í™•ë¥  ë¹„êµë¡œ ë°©ì†¡/ë…¸ì´ì¦ˆ ì±„ë„ ê²°ì •
                if ch0_prob > ch1_prob:
                    print(f"âœ… Sample {b}: Ch0 â†’ Broadcast, Ch1 â†’ Noise")
                    broadcast_channel = sep_acc[b, 0]  # ì±„ë„0ì´ ë°©ì†¡
                    noise_channel = sep_acc[b, 1]      # ì±„ë„1ì´ ë…¸ì´ì¦ˆ
                    is_channel0_broadcast = True
                else:
                    print(f"ğŸ”„ Sample {b}: Ch0 â†’ Noise, Ch1 â†’ Broadcast")
                    broadcast_channel = sep_acc[b, 1]  # ì±„ë„1ì´ ë°©ì†¡
                    noise_channel = sep_acc[b, 0]      # ì±„ë„0ì´ ë…¸ì´ì¦ˆ
                    is_channel0_broadcast = False
                
                batch_broadcast_channels.append(broadcast_channel)
                batch_noise_channels.append(noise_channel)
                
                # ë¶„ë¥˜ ì •ë³´ ì €ì¥
                batch_classification_info.append({
                    'ch0_prob': ch0_prob,
                    'ch1_prob': ch1_prob,
                    'is_channel0_broadcast': is_channel0_broadcast,
                    'confidence': abs(ch0_prob - ch1_prob)
                })
            
            # ë°°ì¹˜ë¡œ ì¬êµ¬ì„±
            sep_broadcast = torch.stack(batch_broadcast_channels).unsqueeze(1)  # (B, 1, T)
            sep_noise = torch.stack(batch_noise_channels).unsqueeze(1)          # (B, 1, T)
            
            classification_results = {
                'batch_info': batch_classification_info,
                'average_accuracy': None
            }
            
            print(f"ğŸ¯ Classification completed for {batch_size} samples")
            
        else:
            print("âš ï¸ No classifier available, using default assignment")
            # ë¶„ë¥˜ê¸° ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì±„ë„ì„ ë°©ì†¡ìœ¼ë¡œ ê°€ì •
            sep_broadcast = sep_acc[:, 0:1]  # (B, 1, T)
            sep_noise = sep_acc[:, 1:2]      # (B, 1, T)

        # === 3ë‹¨ê³„: ANC(WaveNet-VNNs) - ë¶„ë¥˜ëœ ë…¸ì´ì¦ˆ ì±„ë„ì— ì ìš© ===
        print(f"ğŸ”§ Applying ANC to classified noise channels...")
        batch_enhanced = []
        batch_anti_noise = []
        
        for b in range(batch_size):
            # ğŸ”§ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ì±„ë„ ì‚¬ìš©
            noise_sample = sep_noise[b:b+1]  # (1, 1, T) - ì´ì œ ë¶„ë¥˜ëœ ë…¸ì´ì¦ˆ ì±„ë„!
            enhanced_sample, anti_sample = self.reduce_noise_like_inference(noise_sample, device)
            batch_enhanced.append(enhanced_sample)
            batch_anti_noise.append(anti_sample)
        
        # ë°°ì¹˜ë¡œ ì¬êµ¬ì„±
        s2_enhanced = torch.cat(batch_enhanced, dim=0)    # (B, 1, T)
        s2_antinoise = torch.cat(batch_anti_noise, dim=0) # (B, 1, T)

        # === 4ë‹¨ê³„: ì¬í•©ì„± - ë¶„ë¥˜ëœ ë°©ì†¡ ì±„ë„ + ANC ì²˜ë¦¬ëœ ë…¸ì´ì¦ˆ ===
        print(f"ğŸ›ï¸ Final mixing: classified broadcast + enhanced noise")
        final_mix = sep_broadcast + s2_enhanced

        # ê²°ê³¼ ë°˜í™˜
        results = {
            's1_clean': sep_broadcast,        # ë¶„ë¥˜ëœ ë°©ì†¡ ì±„ë„
            's2_noise': sep_noise,            # ë¶„ë¥˜ëœ ë…¸ì´ì¦ˆ ì±„ë„
            's2_target': None,
            's2_antinoise': s2_antinoise,
            's2_enhanced': s2_enhanced,
            'enhanced_verification': final_mix,
            'sep_acc': sep_acc
        }

        # s2_target ê³„ì‚° (ë¶„ë¥˜ëœ ë…¸ì´ì¦ˆ ì±„ë„ ê¸°ë°˜)
        results['s2_target'] = fir_filter(self.pri_filter.to(device), sep_noise)

        # ë¶„ë¥˜ ê²°ê³¼ ì¶”ê°€
        if return_classification and classification_results:
            results['classification'] = classification_results

        return results
    
    def _prepare_for_classification(self, audio):
        """ë¶„ë¥˜ê¸°ìš© ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œë¥¼ í•¨ìˆ˜ë¡œ ë¶„ë¦¬)"""
        # 3ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ê¸°
        while audio.dim() > 3:
            min_dim_idx = audio.shape.index(min(audio.shape[1:-1]))
            audio = audio.squeeze(min_dim_idx + 1)
        
        if audio.dim() == 2:
            audio = audio.unsqueeze(1)
        
        if audio.dim() != 3:
            raise ValueError(f"Expected 3D tensor for classifier, got {audio.dim()}D")
        
        # ê¸¸ì´ ì¡°ì • (BroadcastClassifierëŠ” 16000 ê¸¸ì´ ê¸°ëŒ€)
        current_length = audio.shape[-1]
        if current_length > 16000:
            start_idx = (current_length - 16000) // 2            
            audio = audio[..., start_idx:start_idx + 16000]
        elif current_length < 16000:
            pad_len = 16000 - current_length
            audio = F.pad(audio, (0, pad_len), mode='constant', value=0)
        
        return audio

    def forward_for_training(self, mixed_input, chunk_len=None, return_classification=False):
        """í•™ìŠµìš© forward - ì§ì ‘ í˜¸ì¶œ"""
        return self._forward_direct_safe(mixed_input, chunk_len, return_classification)

    def forward(self, mixed_input, chunk_len=None, return_classification=False):
        """ê¸°ë³¸ forward - training ëª¨ë“œì— ë”°ë¼ ë¶„ê¸°"""
        if self.training:
            return self.forward_for_training(mixed_input, chunk_len, return_classification)
        else:
            return self.forward_inference_style(mixed_input, chunk_len, return_classification)

    def get_trainer_compatible_params(self):
        """íŠ¸ë ˆì´ë„ˆì™€ í˜¸í™˜ë˜ëŠ” íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return {
            'pri_channel': self.pri_channel,
            'sec_channel': self.sec_channel,
            'square_eta': self.square_eta
        }