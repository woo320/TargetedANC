#!/usr/bin/env python3
import os
import sys
import time
import glob
import json

import torch
import numpy as np
import soundfile as sf

# â”€â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¡°ì¸íŠ¸ ëª¨ë¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('/content/drive/MyDrive/joint/code4')
# WaveNet-VNNs ëª¨ë“ˆ ê²½ë¡œ
sys.path.append('/content/drive/MyDrive/WaveNet-VNNs-for-ANC/WaveNet_VNNs')
# sudo_rm_rf ë¶„ë¦¬ ëª¨ë¸ ê²½ë¡œ
sys.path.append('/content/drive/MyDrive/sudo_rm_rf')

# â”€â”€â”€ ì¡°ì¸íŠ¸ ëª¨ë¸ import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from models.joint_model import ImprovedJointModel
from project_utils.audio_utils import standardize_audio_dims
from config.constants import SR

# â”€â”€â”€ ê¸°ì¡´ í•„ìš”í•œ ëª¨ë“ˆë“¤ â”€â”€â”€
from loss_function import dBA_Loss


def load_joint_model(joint_checkpoint_path, device):
    """ì¡°ì¸íŠ¸ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ”§ Loading joint model from: {joint_checkpoint_path}")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(joint_checkpoint_path, map_location=device, weights_only=False)

    # ì„¤ì • ì •ë³´ ì¶”ì¶œ
    if 'config' in checkpoint:
        config = checkpoint['config']
        sudormrf_checkpoint = config['sudormrf_checkpoint']
        wavenet_checkpoint = config['wavenet_checkpoint']
        wavenet_config = config['wavenet_config']
        broadcast_classifier_checkpoint = config.get('broadcast_classifier_checkpoint')
        use_broadcast_classifier = config.get('use_broadcast_classifier', True)
    else:
        # ê¸°ë³¸ ê²½ë¡œë“¤ (ì²´í¬í¬ì¸íŠ¸ì— configê°€ ì—†ëŠ” ê²½ìš°)
        print("âš ï¸ No config in checkpoint, using default paths")
        sudormrf_checkpoint = "/content/drive/MyDrive/joint/weight/.pt"
        wavenet_checkpoint = "/content/drive/MyDrive/joint/weight/reduction.pth"
        wavenet_config = "/content/drive/MyDrive/joint/WaveNet-VNNs-for-ANC/WaveNet_VNNs/config_opt_210.json"
        broadcast_classifier_checkpoint = "/content/drive/MyDrive/joint/weight/classifier.pth"
        use_broadcast_classifier = True

    # ì¡°ì¸íŠ¸ ëª¨ë¸ ìƒì„±
    model = ImprovedJointModel(
        sudormrf_checkpoint_path=sudormrf_checkpoint,
        wavenet_checkpoint_path=wavenet_checkpoint,
        wavenet_config_path=wavenet_config,
        broadcast_classifier_checkpoint_path=broadcast_classifier_checkpoint,
        use_broadcast_classifier=use_broadcast_classifier
    ).to(device)

    # ì¡°ì¸íŠ¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    if 'model_state_dict' in checkpoint:
        model_state = checkpoint['model_state_dict']
    elif 'model' in checkpoint:
        model_state = checkpoint['model']
    else:
        model_state = checkpoint

    # module. ì ‘ë‘ì‚¬ ì œê±° (DataParallel ì‚¬ìš©í–ˆë˜ ê²½ìš°)
    if any(key.startswith('module.') for key in model_state.keys()):
        new_state = {key[7:] if key.startswith('module.') else key: value
                    for key, value in model_state.items()}
        model_state = new_state

    model.load_state_dict(model_state, strict=False)  # strict=Falseë¡œ ì¼ë¶€ ëˆ„ë½ í—ˆìš©
    model.eval()

    print(f"âœ… Joint model loaded successfully")
    if 'epoch' in checkpoint:
        print(f"   Epoch: {checkpoint['epoch']}")
    if 'metrics' in checkpoint:
        metrics = checkpoint['metrics']
        if 'anc_total' in metrics:
            print(f"   ANC Loss: {metrics['anc_total']:.4f}")
        if 'classification_accuracy' in metrics:
            print(f"   Classification Accuracy: {metrics['classification_accuracy']:.3f}")

    return model


def run_joint_inference(
    joint_ckpt,
    input_dir,
    sep_out,
    noise_out,
    anti_out,
    denoise_out,
    final_out,
    use_inference_style=True,
    save_debug=False,
    skip_first_n=3
):
    """ì¡°ì¸íŠ¸ ëª¨ë¸ì„ ì‚¬ìš©í•œ end-to-end ì¶”ë¡  (WaveNet ìŠ¤íƒ€ì¼ NMSE)"""

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(sep_out,      exist_ok=True)
    os.makedirs(noise_out,    exist_ok=True)
    os.makedirs(anti_out,     exist_ok=True)
    os.makedirs(denoise_out,  exist_ok=True)
    os.makedirs(final_out,    exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Using device: {device}")

    # ì¡°ì¸íŠ¸ ëª¨ë¸ ë¡œë“œ
    joint_model = load_joint_model(joint_ckpt, device)

    # dBA ì†ì‹¤ í•¨ìˆ˜ (ì§€í‘œ ê³„ì‚°ìš©)
    try:
        dBA_fn = dBA_Loss(fs=SR, nfft=512, f_up=SR/2).to(device)
    except Exception as e:
        print(f"âš ï¸ dBA Loss function failed: {e}, using simple metric")
        dBA_fn = None

    # ì²­í¬ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
    chunk_len = int(4.0 * SR)  # 4ì´ˆ

    print(f"ğŸ¯ Processing audio files...")
    print(f"   Input directory: {input_dir}")
    print(f"   Chunk length: {chunk_len/SR:.1f} seconds")
    print(f"   BroadcastClassifier: {'Enabled' if joint_model.use_broadcast_classifier else 'Disabled'}")
    print(f"   Inference style: {'Inference' if use_inference_style else 'Training'}")

    # ì…ë ¥ íŒŒì¼ ì°¾ê¸°
    wav_files = sorted(glob.glob(os.path.join(input_dir, "*.wav")))
    if not wav_files:
        print(f"âŒ No .wav files found in {input_dir}")
        return

    metrics = {
        'dBA': [],
        'nmse': [],           # WaveNet ìŠ¤íƒ€ì¼ NMSE
        'si_sdr': [],
        'snr_improvement': [], # WaveNet ìŠ¤íƒ€ì¼ SNR
        'rtf': [],
        'ms_per_s': []        # ğŸ†• ì²˜ë¦¬ ì†ë„ (ms/s) ì¶”ê°€
    }

    for idx, path in enumerate(wav_files):
        fname = os.path.splitext(os.path.basename(path))[0]
        print(f"\nğŸ”„ Processing: {fname}")

        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            wav, fs = sf.read(path)
            if wav.ndim == 2:
                wav = wav.mean(axis=1)  # ëª¨ë…¸ë¡œ ë³€í™˜

            if fs != SR:
                print(f"   âš ï¸ Sample rate mismatch: expected {SR}, got {fs}")
                # ë¦¬ìƒ˜í”Œë§ ì‹œë„ (librosa ì‚¬ìš©)
                try:
                    import librosa
                    wav = librosa.resample(wav, orig_sr=fs, target_sr=SR)
                    print(f"   âœ… Resampled to {SR}Hz")
                except ImportError:
                    print(f"   âŒ Cannot resample (librosa not available)")
                    continue

            t0 = time.time()

            # ì…ë ¥ ì˜¤ë””ì˜¤ ì¤€ë¹„
            audio_tensor = torch.from_numpy(wav).float().to(device)
            audio_tensor = standardize_audio_dims(audio_tensor)  # [1, 1, T] í˜•íƒœë¡œ ë³€í™˜

            print(f"   Input shape: {audio_tensor.shape}")
            print(f"   Audio duration: {len(wav)/SR:.2f} seconds")

            # ğŸš€ ì¡°ì¸íŠ¸ ëª¨ë¸ ì¶”ë¡  (í•œ ë²ˆì— ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!)
            with torch.no_grad():
                if use_inference_style:
                    # ì¶”ë¡  ìŠ¤íƒ€ì¼ (ë” ì•ˆì •ì )
                    outputs = joint_model.forward_inference_style(
                        audio_tensor,
                        chunk_len=chunk_len,
                        return_classification=True
                    )
                else:
                    # í•™ìŠµ ìŠ¤íƒ€ì¼ (ë” ë¹ ë¦„)
                    outputs = joint_model.forward_for_training(
                        audio_tensor,
                        chunk_len=chunk_len,
                        return_classification=True
                    )

            t1 = time.time()

            # ê²°ê³¼ ì¶”ì¶œ
            sep_broadcast = outputs['s1_clean'].squeeze().cpu().numpy()      # ë¶„ë¦¬ëœ ë°©ì†¡
            sep_noise = outputs['s2_noise'].squeeze().cpu().numpy()          # ë¶„ë¦¬ëœ ì†ŒìŒ (ì›ë³¸)
            s2_target = outputs['s2_target'].squeeze().cpu().numpy()         # ANC íƒ€ê²Ÿ
            s2_antinoise = outputs['s2_antinoise'].squeeze().cpu().numpy()   # ANC ìƒì‡„ ì‹ í˜¸
            s2_enhanced = outputs['s2_enhanced'].squeeze().cpu().numpy()     # ANC ì²˜ë¦¬ëœ ì†ŒìŒ
            final_mix = outputs['enhanced_verification'].squeeze().cpu().numpy()  # ìµœì¢… ê²°ê³¼

            # ë¶„ë¥˜ ê²°ê³¼ ì²˜ë¦¬
            classification_info = ""
            if 'classification' in outputs and outputs['classification'] is not None:
                try:
                    classification_results = outputs['classification']
                    
                    if isinstance(classification_results, dict) and 'batch_info' in classification_results:
                        # ì¶”ë¡  ìŠ¤íƒ€ì¼ ê²°ê³¼
                        batch_info = classification_results['batch_info'][0]
                        
                        # âœ… ì˜¬ë°”ë¥¸ í‚¤ ì‚¬ìš©
                        chan0_prob = batch_info['ch0_prob']  # mask_chan0 â†’ ch0_prob
                        chan1_prob = batch_info['ch1_prob']  # mask_chan1 â†’ ch1_prob
                        is_ch0_broadcast = batch_info['is_channel0_broadcast']
                        
                        classification_info = f"Ch0:{chan0_prob:.3f}, Ch1:{chan1_prob:.3f}, Broadcast:Ch{'0' if is_ch0_broadcast else '1'}"
                        
                    elif isinstance(classification_results, torch.Tensor):
                        prob = torch.sigmoid(classification_results).item()
                        classification_info = f"Broadcast prob: {prob:.3f}"
                    else:
                        classification_info = "Classification available"
                        
                except KeyError as e:
                    print(f"âš ï¸ KeyError: {e}")
                    classification_info = "Classification key error"
                except Exception as e:
                    print(f"âš ï¸ Classification error: {e}")
                    classification_info = "Classification failed"

            '''classification_info = ""
                if 'classification' in outputs and outputs['classification'] is not None:
                if isinstance(outputs['classification'], dict) and 'batch_info' in outputs['classification']:
                    # ì¶”ë¡  ìŠ¤íƒ€ì¼ ê²°ê³¼
                    batch_info = outputs['classification']['batch_info'][0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
                    chan0_prob = batch_info['mask_chan0']
                    chan1_prob = batch_info['mask_chan1']
                    is_ch0_broadcast = batch_info['is_channel0_broadcast']
                    classification_info = f"Ch0:{chan0_prob:.3f}, Ch1:{chan1_prob:.3f}, Broadcast:Ch{'0' if is_ch0_broadcast else '1'}"
                elif isinstance(outputs['classification'], torch.Tensor):
                    # í•™ìŠµ ìŠ¤íƒ€ì¼ ê²°ê³¼
                    prob = torch.sigmoid(outputs['classification']).item()
                    classification_info = f"Broadcast prob: {prob:.3f}"'''

            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            total_ms = (t1 - t0) * 1000.0
            audio_sec = len(wav) / SR
            rtf = (total_ms / 1000.0) / audio_sec
            ms_per_s = total_ms / audio_sec  # ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆìŒ

            # ğŸ”„ WaveNet ìŠ¤íƒ€ì¼ ì§€í‘œ ê³„ì‚°
            try:
                if dBA_fn is not None:
                    enh_t = torch.tensor(s2_enhanced, device=device).unsqueeze(0)
                    tgt_t = torch.tensor(s2_target, device=device).unsqueeze(0)

                    dBA_val = (dBA_fn(enh_t) - dBA_fn(tgt_t)).item()

                    # ğŸ¯ WaveNet ìŠ¤íƒ€ì¼ NMSE ê³„ì‚°
                    nmse_val = 10 * torch.log10(
                        torch.sum(enh_t.squeeze() ** 2) /
                        torch.sum(tgt_t.squeeze() ** 2)
                    ).item()

                    # SI-SDR ê³„ì‚° (ê¸°ì¡´ ìœ ì§€)
                    def compute_si_sdr(est, ref):
                        est_zm = est - torch.mean(est)
                        ref_zm = ref - torch.mean(ref)
                        alpha = torch.sum(est_zm * ref_zm) / torch.sum(ref_zm ** 2)
                        ref_scaled = alpha * ref_zm
                        noise = est_zm - ref_scaled
                        sisdr = 10 * torch.log10(torch.sum(ref_scaled ** 2) / (torch.sum(noise ** 2) + 1e-9))
                        return sisdr.item()

                    si_sdr_val = compute_si_sdr(enh_t.squeeze(), tgt_t.squeeze())

                    # ğŸ¯ WaveNet ìŠ¤íƒ€ì¼ SNR ê³„ì‚° (ì¶œë ¥ SNR)
                    snr_improvement = 10 * torch.log10(
                        torch.sum(tgt_t.squeeze() ** 2) /
                        (torch.sum((enh_t.squeeze() - tgt_t.squeeze()) ** 2) + 1e-9)
                    ).item()

                else:
                    # ê°„ë‹¨í•œ numpy ê³„ì‚°
                    nmse_val = 10 * np.log10(np.sum(s2_enhanced ** 2) / np.sum(s2_target ** 2))
                    dBA_val = 20 * np.log10(np.sqrt(np.mean((s2_enhanced - s2_target) ** 2)) + 1e-9)
                    si_sdr_val = 0.0
                    snr_improvement = 0.0

            except Exception as e:
                print(f"   âš ï¸ Metric calculation failed: {e}")
                dBA_val = 0.0
                nmse_val = 0.0
                si_sdr_val = 0.0
                snr_improvement = 0.0

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            sf.write(os.path.join(sep_out,     f"{fname}_broadcast.wav"), sep_broadcast, SR)
            sf.write(os.path.join(noise_out,   f"{fname}_sep_noise.wav"), sep_noise, SR)
            sf.write(os.path.join(anti_out,    f"{fname}_anti_noise.wav"), s2_antinoise, SR)
            sf.write(os.path.join(denoise_out, f"{fname}_denoised_noise.wav"), s2_enhanced, SR)
            sf.write(os.path.join(final_out,   f"{fname}_final.wav"), final_mix, SR)

            # âœ… metrics ì €ì¥ (ì¤‘ë³µ ì œê±° - í•œ ë²ˆë§Œ!)
            if idx >= skip_first_n:
                metrics['dBA'].append(dBA_val)
                metrics['nmse'].append(nmse_val)        # WaveNet ìŠ¤íƒ€ì¼
                metrics['si_sdr'].append(si_sdr_val)
                metrics['snr_improvement'].append(snr_improvement)  # WaveNet ìŠ¤íƒ€ì¼
                metrics['rtf'].append(rtf)
                metrics['ms_per_s'].append(ms_per_s)    # ğŸ†• ì²˜ë¦¬ ì†ë„ ì¶”ê°€

            # ì¶”ê°€ ì €ì¥ (ë””ë²„ê¹…ìš©)
            if save_debug:
                debug_dir = os.path.join(final_out, "debug")
                os.makedirs(debug_dir, exist_ok=True)
                sf.write(os.path.join(debug_dir, f"{fname}_s2_target.wav"), s2_target, SR)
                sf.write(os.path.join(debug_dir, f"{fname}_input.wav"), wav, SR)

            print(f"âœ… {fname}: {total_ms:.1f}ms | RTF {rtf:.3f} | {ms_per_s:.1f}ms/s")
            print(f"   dBA: {dBA_val:+.2f}dB | NMSE: {nmse_val:+.2f}dB | SI-SDR: {si_sdr_val:+.2f}dB")
            print(f"   Output SNR: {snr_improvement:+.2f}dB")
            if classification_info:
                print(f"   Classification: {classification_info}")

        except Exception as e:
            print(f"âŒ Error processing {fname}: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\nğŸ‰ Joint model inference completed!")

    if len(metrics['dBA']) > 0:
        print(f"\nğŸ“ˆ PERFORMANCE METRICS (Average of {len(metrics['dBA'])} files)")
        print(f"   (Skipped first {skip_first_n} files)")
        print("=" * 60)  # ê¸¸ì´ ì¡°ê¸ˆ ëŠ˜ë¦¼

        avg_dBA = sum(metrics['dBA']) / len(metrics['dBA'])
        avg_nmse = sum(metrics['nmse']) / len(metrics['nmse'])
        avg_si_sdr = sum(metrics['si_sdr']) / len(metrics['si_sdr'])
        avg_snr_improvement = sum(metrics['snr_improvement']) / len(metrics['snr_improvement'])
        avg_rtf = sum(metrics['rtf']) / len(metrics['rtf'])
        avg_ms_per_s = sum(metrics['ms_per_s']) / len(metrics['ms_per_s'])  # ğŸ†• ì²˜ë¦¬ ì†ë„ í‰ê· 

        # ğŸ¯ Audio Quality Metrics
        print(f"ğŸ¯ Audio Quality Metrics:")
        print(f"   Average dBA:             {avg_dBA:+7.3f} dB")
        print(f"   Average NMSE (WaveNet):  {avg_nmse:+7.3f} dB")
        print(f"   Average SI-SDR:          {avg_si_sdr:+7.3f} dB")
        print(f"   Average Output SNR:      {avg_snr_improvement:+7.3f} dB")

        # âš¡ Performance Metrics
        print(f"\nâš¡ Performance Metrics:")
        print(f"   Average RTF:             {avg_rtf:7.4f}")
        print(f"   Average Processing Speed: {avg_ms_per_s:6.1f} ms/s")  # ğŸ†• ì¶”ê°€

        # ğŸ“Š Range Statistics (ì²˜ë¦¬ ì†ë„ ë²”ìœ„ë„ ì¶”ê°€)
        print(f"\nğŸ“Š Range Statistics:")
        print(f"   dBA Range:      {min(metrics['dBA']):+.3f} ~ {max(metrics['dBA']):+.3f} dB")
        print(f"   NMSE Range:     {min(metrics['nmse']):+.3f} ~ {max(metrics['nmse']):+.3f} dB")
        print(f"   SI-SDR Range:   {min(metrics['si_sdr']):+.3f} ~ {max(metrics['si_sdr']):+.3f} dB")
        print(f"   RTF Range:      {min(metrics['rtf']):.4f} ~ {max(metrics['rtf']):.4f}")
        print(f"   Speed Range:    {min(metrics['ms_per_s']):.1f} ~ {max(metrics['ms_per_s']):.1f} ms/s")  # ğŸ†• ì¶”ê°€

        print("=" * 60)

        # ğŸ†• JSON í˜•íƒœë„ ì—…ë°ì´íŠ¸
        summary = {
            'files_processed': len(wav_files),
            'files_for_average': len(metrics['dBA']),
            'skipped_files': skip_first_n,
            'audio_quality': {
                'dBA_dB': round(avg_dBA, 3),
                'nmse_dB': round(avg_nmse, 3),
                'si_sdr_dB': round(avg_si_sdr, 3),
                'output_snr_dB': round(avg_snr_improvement, 3)
            },
            'performance': {
                'rtf': round(avg_rtf, 4),
                'processing_speed_ms_per_s': round(avg_ms_per_s, 1)  # ğŸ†• ì¶”ê°€
            }
        }

        print(f"\nğŸ“‹ Summary (JSON format):")
        import json
        print(json.dumps(summary, indent=2))

    print(f"   Processed {len(wav_files)} files")
    print(f"   Output directories:")
    print(f"     - Broadcast: {sep_out}")
    print(f"     - Noise: {noise_out}")
    print(f"     - Anti-noise: {anti_out}")
    print(f"     - Denoised: {denoise_out}")
    print(f"     - Final: {final_out}")


# âš¡ Colabìš© ê°„í¸ í•¨ìˆ˜ë“¤
def quick_inference(joint_model_path, input_audio_dir, output_base_dir, skip_first=3):
    """ê°€ì¥ ê°„ë‹¨í•œ ì¶”ë¡  ì‹¤í–‰"""

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
    output_dirs = {
        'sep_out': os.path.join(output_base_dir, "broadcast"),
        'noise_out': os.path.join(output_base_dir, "noise"),
        'anti_out': os.path.join(output_base_dir, "antinoise"),
        'denoise_out': os.path.join(output_base_dir, "denoised"),
        'final_out': os.path.join(output_base_dir, "final")
    }

    print("ğŸš€ Quick Joint Model Inference")
    print("=" * 50)
    print(f"Model: {joint_model_path}")
    print(f"Input: {input_audio_dir}")
    print(f"Output: {output_base_dir}")
    print("=" * 50)

    run_joint_inference(
        joint_ckpt=joint_model_path,
        input_dir=input_audio_dir,
        use_inference_style=True,
        save_debug=True,
        skip_first_n=skip_first,  #ìŠ¤
        **output_dirs
    )


# ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ì‚¬ìš©ì‹œ ìˆ˜ì • í•„ìš”)
    JOINT_MODEL_PATH = "/content/drive/MyDrive/joint/result/joint_mixed_training_2025-06-09-12h00m/weights/best_composite.pth"
    INPUT_DIR = "/content/drive/MyDrive/final_data/á„Œá…µá„’á…¡á„á…¥á†¯á„‰á…µá†¯á„Œá…¦á„‚á…©á†¨á„‹á…³á†·"
    OUTPUT_DIR = "/content/drive/MyDrive/joint/inference_output"

    # ì‹¤í–‰
    quick_inference(JOINT_MODEL_PATH, INPUT_DIR, OUTPUT_DIR)