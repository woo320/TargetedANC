import os
import sys
import time
import torch
import torch.optim as optim
import numpy as np
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader
from tqdm import tqdm

# SudoRM-RF PIT ì†ì‹¤í•¨ìˆ˜ import
try:
    from asteroid.losses import PITLossWrapper, pairwise_neg_sisdr
    ASTEROID_AVAILABLE = True
except ImportError:
    ASTEROID_AVAILABLE = False
    print("âš ï¸ Asteroid library not available - using basic SI-SDR")

from models.model_utils import compute_broadcast_classification_loss
# WaveNet ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, "/content/drive/MyDrive/joint/WaveNet-VNNs-for-ANC/WaveNet_VNNs")
from loss_function import dBA_Loss, NMSE_Loss
from utils import fir_filter, SEF

# ë¡œì»¬ ëª¨ë“ˆ import
from datasets.sudormrf_dataset import SudoRMRFDynamicMixDataset
from datasets.improved_dataset import ImprovedAudioDataset
from datasets.collate_functions import sudormrf_dynamic_mix_collate_fn, improved_collate_fn
from .memory_manager import MemoryManager
from project_utils import EarlyStoppingManager
from project_utils.audio_utils import standardize_audio_dims
from project_utils.augmentation import online_augment_sudormrf
from config.constants import SR

class ImprovedJointTrainerWithSudoRMRFMix:

    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.memory_manager = MemoryManager(config)
        self.accumulation_steps = config.get('accumulation_steps', 4)
        
        # SudoRM-RF ì„¤ì •
        self.use_online_augment = config.get('use_online_augment', True)
        self.use_pit_loss = config.get('use_pit_loss', True) and ASTEROID_AVAILABLE

        self._setup_paths()
        self._setup_model()
        self._setup_dataloaders()
        self._setup_anc_paths()
        self._setup_training()

        if config.get('use_tensorboard', True):
            self.writer = SummaryWriter(self.log_path)

        pit_status = "with PIT loss" if self.use_pit_loss else "with basic SI-SDR"
        print(f"Joint Trainer Mixed Training (Training:Dynamic + Val:Premixed) {pit_status} initialized")

    def _setup_paths(self):
        timestamp = datetime.now().strftime("%Y-%m-%d-%Hh%Mm")
        mix_suffix = "_mixed_training"  # Training(ë™ì ) + Validation(premixed)
        self.exp_path = f"/content/drive/MyDrive/joint/result/joint{mix_suffix}_{timestamp}"
        self.log_path = os.path.join(self.exp_path, 'logs')
        self.checkpoint_path = os.path.join(self.exp_path, 'weights')

        os.makedirs(self.log_path, exist_ok=True)
        os.makedirs(self.checkpoint_path, exist_ok=True)

    def _setup_model(self):
        from models.joint_model import ImprovedJointModel
        self.model = ImprovedJointModel(
            sudormrf_checkpoint_path=self.config['sudormrf_checkpoint'],
            wavenet_checkpoint_path=self.config['wavenet_checkpoint'],
            wavenet_config_path=self.config['wavenet_config'],
            broadcast_classifier_checkpoint_path=self.config.get('broadcast_classifier_checkpoint'),
            use_broadcast_classifier=self.config.get('use_broadcast_classifier', False),
            model_config=self.config
        ).to(self.device)

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Model loaded with {trainable_params:,} trainable parameters")

    def _setup_dataloaders(self):
        """ë°ì´í„°ë¡œë” ì„¤ì • - configë¥¼ collate_fnì— ì „ë‹¬"""
        dataloaders = {}

        for split in ['train', 'val', 'test']:
            split_dir = os.path.join(self.config['dataset_root'], split)

            if not os.path.exists(split_dir):
                dataloaders[split] = None
                continue

            try:
                # ìƒ˜í”Œ ìˆ˜ ì œí•œ
                max_samples = None
                if split == 'train' and self.config.get('limit_train_samples'):
                    max_samples = self.config['limit_train_samples']
                elif split == 'val' and self.config.get('limit_val_samples'):                    
                    max_samples = self.config['limit_val_samples']
                elif split == 'test' and self.config.get('limit_test_samples'):
                    max_samples = self.config['limit_test_samples']

                # splitë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„
                if split == 'train':
                    # Training: ë¬´ì¡°ê±´ ë™ì  ë¯¹ìŠ¤
                    dataset = SudoRMRFDynamicMixDataset(
                        self.config['dataset_root'],
                        split=split,
                        max_samples=max_samples,
                        max_duration=self.config.get('max_audio_duration', 15.0),
                        use_online_augment=self.use_online_augment
                    )
                    collate_fn = lambda batch: sudormrf_dynamic_mix_collate_fn(batch, self.config)
                    
                else:
                    # Validation/Test: ë¬´ì¡°ê±´ pre-mixed
                    dataset = ImprovedAudioDataset(
                        self.config['dataset_root'],
                        split=split,
                        max_samples=max_samples,
                        max_duration=self.config.get('max_audio_duration', 15.0)
                    )
                    collate_fn = lambda batch: improved_collate_fn(batch, self.config)

                # ì ì‘ì  ë°°ì¹˜ í¬ê¸°
                batch_size = self.memory_manager.adaptive_batch_size(
                    self.config.get('batch_size', 1)
                )

                if split in ['validation', 'test']:
                    batch_size = min(batch_size * 2, self.config.get('batch_size', 1) * 2)

                dataloader = DataLoader(
                    dataset,
                    batch_size=batch_size,
                    shuffle=(split == 'train'),
                    num_workers=self.config.get('dataloader_num_workers', 0),
                    pin_memory=self.config.get('dataloader_pin_memory', False),
                    drop_last=(split == 'train' and self.config.get('dataloader_drop_last', True)),
                    collate_fn=collate_fn
                )

                dataloaders[split] = dataloader

            except Exception as e:
                print(f"Failed to create {split} dataloader: {e}")
                dataloaders[split] = None

        self.train_loader = dataloaders['train']
        self.val_loader = dataloaders['val']
        self.test_loader = dataloaders['test']

    def _setup_anc_paths(self):
        params = self.model.get_trainer_compatible_params()
        self.pri_channel = params['pri_channel']
        self.sec_channel = params['sec_channel'] 
        self.square_eta = params['square_eta']

    def _setup_training(self):
        self.model.train()
        
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ì„¤ì •
        separation_params = list(self.model.separation_model.parameters())
        noise_params = list(self.model.noise_reduction_model.parameters())
        eta_params = [self.model.eta] if hasattr(self.model, 'eta') else []

        classifier_params = []
        if hasattr(self.model, 'broadcast_classifier') and self.model.broadcast_classifier is not None:
            classifier_params = list(self.model.broadcast_classifier.parameters())

        param_groups = [
            {'params': separation_params, 'lr': self.config['separation_lr']},
            {'params': noise_params, 'lr': self.config['noise_lr']},
        ]
        if eta_params:
            param_groups.append({'params': eta_params, 'lr': self.config['eta_lr']})
        
        if classifier_params:
            param_groups.append({
                'params': classifier_params, 
                'lr': self.config['classifier_lr']
            })

        self.optimizer = optim.Adam(param_groups)

        # ìˆ˜ì •ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        scheduler_params = {
            'mode': 'min',
            'factor': 0.8,                     # 0.7 â†’ 0.8 (ëœ ê¸‰ê²©í•˜ê²Œ)
            'patience': 4,                     # 2 â†’ 4 (ë” ì¸ë‚´ì‹¬ ìˆê²Œ, ì¡°ì¸íŠ¸ í•™ìŠµ ê³ ë ¤)
            'min_lr': 5e-8,                    # 1e-7 â†’ 5e-8 (ë” ë‚®ì€ ìµœì†Œê°’)
            'threshold': 0.001,                # ìƒˆë¡œ ì¶”ê°€: ì˜ë¯¸ìˆëŠ” ê°œì„ ë§Œ ì¸ì •
            'threshold_mode': 'rel'            # ìƒëŒ€ì  ê°œì„ 
        }
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, **scheduler_params
        )

        # ì†ì‹¤ í•¨ìˆ˜ë“¤
        self.bce_loss = torch.nn.BCEWithLogitsLoss().to(self.device)
        
        # SudoRM-RF PIT ì†ì‹¤í•¨ìˆ˜
        if self.use_pit_loss and ASTEROID_AVAILABLE:
            self.sudormrf_loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from='pw_mtx')
        else:
            self.sudormrf_loss_func = None
        
        self.best_val_loss = float('inf')

        # ìˆ˜ì •ëœ ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        if 'early_stopping_patience' not in self.config or 'min_delta' not in self.config:
            raise ValueError("early_stopping_patienceì™€ min_deltaëŠ” configì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤!")

        self.early_stopping = EarlyStoppingManager(
            patience=self.config['early_stopping_patience'],     
            min_delta=self.config['min_delta'],
            metric_weights=self.config.get('early_stopping_weights', {
                'anc_total': 0.4,
                'separation_loss': 0.3, 
                'classification_accuracy': 0.2,
                'final_quality': 0.1
            })
)

    def _sisdr_loss_basic(self, est, target, zero_mean=True, eps=1e-9):
        """ê¸°ë³¸ SI-SDR loss"""
        if est.dim() == 3:
            est = est.squeeze(1)
        if target.dim() == 3:
            target = target.squeeze(1)

        if zero_mean:
            est = est - est.mean(dim=-1, keepdim=True)
            target = target - target.mean(dim=-1, keepdim=True)

        dot = torch.sum(est * target, dim=-1, keepdim=True)
        energy = torch.sum(target**2, dim=-1, keepdim=True) + eps
        proj = (dot / energy) * target
        noise = est - proj

        sisdr = 10 * torch.log10((proj.pow(2).sum(-1) + eps) / (noise.pow(2).sum(-1) + eps))
        return -sisdr.mean()

    def _compute_separation_loss_sudormrf_style(self, estimated_sources, target_sources):
        """SudoRM-RF ìŠ¤íƒ€ì¼ ë¶„ë¦¬ ì†ì‹¤ ê³„ì‚°"""
        # ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°
        if estimated_sources.shape != target_sources.shape:
            min_batch = min(estimated_sources.shape[0], target_sources.shape[0])
            estimated_sources = estimated_sources[:min_batch]
            target_sources = target_sources[:min_batch]
            
            min_time = min(estimated_sources.shape[-1], target_sources.shape[-1])
            estimated_sources = estimated_sources[..., :min_time]
            target_sources = target_sources[..., :min_time]
        
        if self.use_pit_loss and self.sudormrf_loss_func is not None:
            try:
                if (estimated_sources.shape == target_sources.shape and 
                    estimated_sources.requires_grad and 
                    target_sources.dtype == estimated_sources.dtype and
                    len(estimated_sources.shape) == 3 and
                    estimated_sources.shape[1] == 2):
                    
                    pit_loss = self.sudormrf_loss_func(estimated_sources, target_sources)
                    
                    if torch.isnan(pit_loss) or torch.isinf(pit_loss):
                        raise ValueError("Invalid PIT loss")
                    
                    return pit_loss
                else:
                    raise ValueError("PIT requirements not met")
                    
            except Exception:
                # Fallback to basic SI-SDR
                s1_loss = self._sisdr_loss_basic(estimated_sources[:, 0], target_sources[:, 0])
                s2_loss = self._sisdr_loss_basic(estimated_sources[:, 1], target_sources[:, 1])
                return (s1_loss + s2_loss) / 2
        else:
            # ê¸°ë³¸ SI-SDR ì‚¬ìš©
            s1_loss = self._sisdr_loss_basic(estimated_sources[:, 0], target_sources[:, 0])
            s2_loss = self._sisdr_loss_basic(estimated_sources[:, 1], target_sources[:, 1])
            return (s1_loss + s2_loss) / 2 
        
    def safe_dba_loss(self, signal, target=None, eps=1e-8):
        """ì¶”ë¡ ì½”ë“œì™€ ë™ì¼í•œ dBA ê³„ì‚° (ì°¨ì´ ë°©ì‹)"""
        try:
            if target is not None:
                # ì¶”ë¡ ì½”ë“œì™€ ë™ì¼: dBA(enhanced) - dBA(target)
                signal_power = torch.mean(signal ** 2)
                target_power = torch.mean(target ** 2)
                
                signal_db = 10 * torch.log10(signal_power + eps)
                target_db = 10 * torch.log10(target_power + eps)
                
                dba_diff = signal_db - target_db
                return dba_diff
            else:
                # ë‹¨ì¼ ì‹ í˜¸ì˜ íŒŒì›Œ
                power = torch.mean(signal ** 2)
                db_power = 10 * torch.log10(power + eps)
                return db_power
                    
        except Exception:
            if target is not None:
                return torch.mean((signal - target) ** 2)
            else:
                return torch.mean(signal ** 2)

    def safe_nmse_loss(self, signal, target, eps=1e-8):
        """ìˆ˜ì •ëœ NMSE ê³„ì‚°"""
        try:
            # ì°¨ì› í™•ì¸
            if signal.dim() != target.dim():
                signal = signal.view(-1)
                target = target.view(-1)
            
            # ê¸¸ì´ ë§ì¶”ê¸°
            min_len = min(signal.shape[-1], target.shape[-1])
            signal = signal[..., :min_len]
            target = target[..., :min_len]
            
            # ì •í™•í•œ NMSE ê³„ì‚°
            signal_power = torch.sum(signal ** 2) + eps
            target_power = torch.sum(target ** 2) + eps
            
            # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            if target_power < eps * 10:
                return torch.mean((signal - target) ** 2)
            
            nmse_linear = signal_power / target_power
            nmse_db = 10 * torch.log10(nmse_linear + eps)
            
            # ë²”ìœ„ ì œí•œ (-50dB ~ +50dB)
            nmse_db = torch.clamp(nmse_db, -30.0, 30.0)
            
            return nmse_db
            
        except Exception as e:
            print(f"NMSE calculation failed: {e}")
            return torch.mean((signal - target) ** 2)
    
    def _compute_losses(self, outputs, batch):
        # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
        if 'input' in batch:
            input_mixed = batch['input'].to(self.device)
        elif 'sources' in batch:
            sources = batch['sources'].to(self.device)
            input_mixed = sources.sum(dim=1, keepdim=True)
        else:
            raise ValueError("No valid input found in batch")

        target_s1 = batch['separation_targets']['s1'].to(self.device)
        target_s2 = batch['separation_targets']['s2'].to(self.device)
        
        # ì°¨ì› í‘œì¤€í™”
        input_mixed = standardize_audio_dims(input_mixed)
        target_s1 = standardize_audio_dims(target_s1)
        target_s2 = standardize_audio_dims(target_s2)
        
        # ë°°ì¹˜ í¬ê¸° ë§ì¶”ê¸°
        batch_sizes = [input_mixed.shape[0], target_s1.shape[0], target_s2.shape[0]]
        min_batch_size = min(batch_sizes)
        
        if len(set(batch_sizes)) > 1:
            input_mixed = input_mixed[:min_batch_size]
            target_s1 = target_s1[:min_batch_size]
            target_s2 = target_s2[:min_batch_size]

        # ëª¨ë¸ ì¶œë ¥
        s1_clean = outputs['s1_clean']
        s2_noise = outputs['s2_noise']
        s2_antinoise = outputs['s2_antinoise']
        s2_enhanced = outputs['s2_enhanced']
        enhanced_verification = outputs['enhanced_verification']

        # ë¶„ë¥˜ ì†ì‹¤ ê³„ì‚°
        classification_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        classification_accuracy = 0.0
        accuracy_weight = 1.0
        
        if 'classification' in outputs and outputs['classification'] is not None:
            try:
                classification_results = outputs['classification']
                
                if isinstance(classification_results, torch.Tensor):
                    # íƒ€ê²Ÿ ìƒì„±: s1ê³¼ s2ì˜ ì—ë„ˆì§€ ë¹„êµ
                    s1_energy = torch.mean(s1_clean.abs(), dim=-1)
                    s2_energy = torch.mean(s2_noise.abs(), dim=-1)
                    targets = (s1_energy > s2_energy).float().squeeze(-1)
                    
                    classification_loss = self.bce_loss(
                        classification_results.squeeze(-1) if classification_results.dim() > 1 else classification_results,
                        targets
                    )

                    probs = torch.sigmoid(classification_results.squeeze())
                    predicted = (probs > 0.5).float()
                    classification_accuracy = (predicted == targets).float().mean().item()
                
                accuracy_weight = 1.0 + (1.0 - classification_accuracy) * 0.5
                
            except Exception:
                classification_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
                classification_accuracy = 0.0
                accuracy_weight = 1.0
        
        # batch íƒ€ì…ìœ¼ë¡œ Training/Validation êµ¬ë¶„
        if 'sources' in batch:
            # Training: SudoRM-RF ìŠ¤íƒ€ì¼ PIT ì†ì‹¤
            estimated_sources = torch.stack([
                s1_clean.squeeze(1),
                s2_noise.squeeze(1)
            ], dim=1)
            
            target_sources = torch.stack([
                target_s1.squeeze(1),
                target_s2.squeeze(1)
            ], dim=1).float()
            
            separation_loss = self._compute_separation_loss_sudormrf_style(
                estimated_sources, target_sources
            ) * accuracy_weight
            
            s1_separation_loss = separation_loss / 2  # ë¡œê¹…ìš© ê·¼ì‚¬ê°’
            s2_separation_loss = separation_loss / 2  # ë¡œê¹…ìš© ê·¼ì‚¬ê°’
            separation_method = "PIT-SISDR" if self.use_pit_loss else "SI-SDR Average"
            
        else:
            # Validation/Test: ê°œë³„ SI-SDR ê³„ì‚°
            s1_separation_loss = self._sisdr_loss_basic(s1_clean, target_s1) * accuracy_weight
            s2_separation_loss = self._sisdr_loss_basic(s2_noise, target_s2) * accuracy_weight
            separation_loss = (s1_separation_loss + s2_separation_loss) / 2
            separation_method = "Individual SI-SDR"

        # ì•ˆì „í•œ ANC ì†ì‹¤ ê³„ì‚° (ì¶”ë¡  ë°©ì‹ìœ¼ë¡œ í†µì¼)
        try:
            target_flat = outputs['s2_target'].squeeze(1)
            en_flat = outputs['s2_enhanced'].squeeze(1)
            
            # dBA: enhanced_dB - target_dB
            anc_dba_loss = self.safe_dba_loss(en_flat, target_flat)
            
            # NMSE: 10 * log10(enhanced_power / target_power)
            anc_nmse_loss = self.safe_nmse_loss(en_flat, target_flat)
            
            # ê³„ì‚° í›„ì— ë””ë²„ê·¸ ì¶œë ¥ (ìˆ˜ì •ë¨)
            debug_prob = self.config.get('debug_loss_print_prob', 0.05)
            if torch.rand(1).item() < debug_prob:
                print(f"ANC Debug: dBA={anc_dba_loss.item():.4f}dB, "
                        f"NMSE={anc_nmse_loss.item():.4f}dB, "
                        f"Signal_std={en_flat.std().item():.6f}, "
                        f"Target_std={target_flat.std().item():.6f}")
            
            # NaN/Inf ê²€ì¦
            if torch.isnan(anc_dba_loss) or torch.isinf(anc_dba_loss):
                anc_dba_loss = torch.mean((en_flat - target_flat) ** 2)
            
            if torch.isnan(anc_nmse_loss) or torch.isinf(anc_nmse_loss):
                anc_nmse_loss = torch.mean((en_flat - target_flat) ** 2)
            
            # Gradient ë³´ì¥
            if not anc_dba_loss.requires_grad:
                anc_dba_loss = anc_dba_loss.requires_grad_(True)
            if not anc_nmse_loss.requires_grad:
                anc_nmse_loss = anc_nmse_loss.requires_grad_(True)
                
        except Exception as e:
            print(f"ANC loss calculation failed: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            anc_dba_loss = torch.mean((en_flat - target_flat) ** 2).requires_grad_(True)
            anc_nmse_loss = torch.mean((en_flat - target_flat) ** 2).requires_grad_(True)

        # ìµœì¢… í’ˆì§ˆ ì†ì‹¤
        final_quality_loss = self._sisdr_loss_basic(enhanced_verification, target_s1)

        # ì•ˆí‹°ë…¸ì´ì¦ˆ ì œì•½
        antinoise_magnitude = torch.mean(torch.abs(s2_antinoise))
        target_antinoise_magnitude = self.config.get('target_antinoise_magnitude', 0.1)
        antinoise_constraint = torch.abs(antinoise_magnitude - target_antinoise_magnitude)

        # ANC ì´ ì†ì‹¤
        anc_weights = self.config.get('anc_loss_weights', {
            'dba_weight': 0.5,
            'nmse_weight': 0.5
        })
        anc_total_loss = (anc_weights['dba_weight'] * anc_dba_loss + 
                         anc_weights['nmse_weight'] * anc_nmse_loss)

        # ğŸ”§ ê°„ë‹¨í•œ ì´ ì†ì‹¤ ê³„ì‚°
        loss_weights = self.config.get('loss_weights', {
            'final_quality': 0.12,
            'anc_total': 0.28,
            'separation': 0.42,
            'classification': 0.15,
            'antinoise_constraint': 0.03
        })
        
        # ëª¨ë“  ê²½ìš°ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì ìš©
        total_loss = (
            loss_weights['final_quality'] * final_quality_loss +
            loss_weights['anc_total'] * anc_total_loss +
            loss_weights['separation'] * separation_loss +
            loss_weights['classification'] * classification_loss +
            loss_weights['antinoise_constraint'] * antinoise_constraint
        )

        # ìµœì¢… ì†ì‹¤ ê²€ì¦
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            total_loss = final_quality_loss
            
        if not total_loss.requires_grad:
            print("Total loss does not require gradients!")

        return {
            'total_loss': total_loss,
            'anc_dba_loss': anc_dba_loss,
            'anc_nmse_loss': anc_nmse_loss,
            'anc_total_loss': anc_total_loss,
            'final_quality_loss': final_quality_loss,
            's1_separation_loss': s1_separation_loss,
            's2_separation_loss': s2_separation_loss,
            'separation_loss': separation_loss,
            'antinoise_constraint': antinoise_constraint,
            'classification_loss': classification_loss,
            'classification_accuracy': classification_accuracy,
            'use_pit': self.use_pit_loss,
            'separation_method': separation_method
        }

    def _train_epoch(self, epoch):
        """í›ˆë ¨ ì—í¬í¬ (ìˆ˜ì •ëœ ì†ì‹¤ ë¡œê¹…)"""
        self.model.train()

        total_losses = {
            'total': 0, 'anc_dba': 0, 'anc_nmse': 0, 'anc_total': 0,
            'final_quality': 0, 's1_separation': 0, 's2_separation': 0,
            'antinoise_constraint': 0, 'classification': 0
        }
        
        classification_accuracies = []
        aug_stats = []  # Trainingì—ì„œë§Œ ì‚¬ìš©

        num_batches = len(self.train_loader)
        pit_status = " (PIT)" if self.use_pit_loss else " (Basic)"
        train_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}{pit_status}', ncols=150)

        self.optimizer.zero_grad()

        for step, batch in enumerate(train_bar, 1):
            # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
            allocated, reserved = self.memory_manager.get_memory_usage()

            try:
                memory_warning_threshold = self.config.get('memory_warning_threshold', 20.0)
                if step % 20 == 0 and allocated > memory_warning_threshold:
                    self.memory_manager.cleanup_memory(aggressive=True)

                # ë°ì´í„° ì „ì²˜ë¦¬
                if 'sources' in batch:
                    # Training: ë™ì  ë¯¹ìŠ¤
                    sources = batch['sources'].to(self.device)

                    if self.use_online_augment:
                        sources_aug = online_augment_sudormrf(sources)
                        aug_ratio = torch.mean(sources_aug / (sources + 1e-9)).item()
                        aug_stats.append(aug_ratio)
                    else:
                        sources_aug = sources

                    mixed_input = sources_aug.sum(dim=1, keepdim=True)
                    
                    m = mixed_input.mean(dim=-1, keepdim=True)
                    s = mixed_input.std(dim=-1, keepdim=True)
                    mixed_input = (mixed_input - m) / (s + 1e-9)
                else:
                    # Validation: premixed
                    mixed_input = batch['input'].to(self.device)

                mixed_input.requires_grad_(True)

                # ì ì‘ì  ì²­í¬ í¬ê¸°
                adaptive_chunk = self.memory_manager.get_safe_chunk_size()

                # ëª¨ë¸ forward
                outputs = self.model.forward_for_training(
                    mixed_input, 
                    chunk_len=adaptive_chunk, 
                    return_classification=True
                )
                
                # ì†ì‹¤ ê³„ì‚°
                losses = self._compute_losses(outputs, batch)
                
                # ì‹¤ì œ ì—­ì „íŒŒì— ì‚¬ìš©ë  ì†ì‹¤ê°’ ê³„ì‚°
                total_loss_for_backprop = losses['total_loss'] / self.accumulation_steps

            except Exception as e:
                print(f"Error in step {step}: {e}")
                self.optimizer.zero_grad()
                continue

            # Backward ì²˜ë¦¬
            try:
                if torch.isnan(total_loss_for_backprop) or torch.isinf(total_loss_for_backprop) or not total_loss_for_backprop.requires_grad:
                    continue

                total_loss_for_backprop.backward()

                # Gradient accumulation
                if step % self.accumulation_steps == 0:
                    max_grad_norm = self.config.get('max_grad_norm', 0.1)
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=max_grad_norm)
                    
                    if torch.isfinite(grad_norm):
                        self.optimizer.step()
                    
                    self.optimizer.zero_grad()

            except Exception as e:
                print(f"Error in backward pass: {e}")
                self.optimizer.zero_grad()
                continue

            # ìˆ˜ì •ëœ ì†ì‹¤ ë¡œê¹… - ì‹¤ì œ í•™ìŠµì— ì‚¬ìš©ëœ ê°’ë“¤ì„ ë¡œê¹…
            # ì´ ì†ì‹¤ì€ accumulationìœ¼ë¡œ ë‚˜ëˆˆ ê°’
            total_losses['total'] += losses['total_loss'].item()

            for key in ['anc_dba', 'anc_nmse', 'anc_total', 'final_quality', 
                      's1_separation', 's2_separation', 'antinoise_constraint', 'classification']:
                loss_key = key + '_loss' if key != 'total' else 'total_loss'
                if key == 'anc_total':
                    loss_key = 'anc_total_loss'
                elif key == 'classification':
                    loss_key = 'classification_loss'

                if loss_key in losses:
                    actual_loss = losses[loss_key].item()
                    total_losses[key] += actual_loss
            
            if 'classification_accuracy' in losses:
                classification_accuracies.append(losses['classification_accuracy'])

            avg_total = total_losses['total'] / step
            avg_anc = total_losses['anc_total'] / step
            avg_separation = (total_losses['s1_separation'] + total_losses['s2_separation']) / 2 / step
            avg_classification = total_losses['classification'] / step

            postfix = {
            'Total': f"{avg_total:.4f}",
            'ANC': f"{avg_anc:.2f}dB", 
            'Sep': f"{avg_separation:.4f}",
            'Cls': f"{avg_classification:.4f}"
            }

            # ë¶„ë¥˜ ì •í™•ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if classification_accuracies:
                recent_acc = np.mean(classification_accuracies[-20:])  # ìµœê·¼ 20ê°œ í‰ê· 
                postfix['Acc'] = f"{recent_acc:.2f}"
            
            # ë©”ëª¨ë¦¬ (ê°„ë‹¨íˆ)
            postfix['Mem'] = f"{allocated:.1f}G"

            train_bar.set_postfix(postfix)

            # ìƒì„¸ ì •ë³´ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ ë³„ë„ ì¶œë ¥ (ë§¤ 50ìŠ¤í…)
            if step % 50 == 0:
                # ìƒì„¸ ë¶„ì„
                avg_s1 = total_losses['s1_separation'] / step
                avg_s2 = total_losses['s2_separation'] / step
                avg_final_quality = total_losses['final_quality'] / step
                avg_anc_dba = total_losses['anc_dba'] / step
                avg_anc_nmse = total_losses['anc_nmse'] / step
                
                print(f"\nStep {step}/{num_batches} Detailed Metrics:")
                print(f"Separation: S1={avg_s1:.4f}, S2={avg_s2:.4f}")
                print(f"ANC: dBA={avg_anc_dba:.2f}dB, NMSE={avg_anc_nmse:.2f}dB")
                print(f"Final Quality: {avg_final_quality:.4f}")
                
                if classification_accuracies:
                    recent_acc = np.mean(classification_accuracies[-50:])
                    print(f"Classification: Loss={avg_classification:.4f}, Acc={recent_acc:.1%}")
                
                # í•™ìŠµ ìƒíƒœ ìš”ì•½
                method_info = []
                if self.use_pit_loss:
                    method_info.append("PIT")
                if self.use_online_augment and aug_stats:
                    avg_aug = np.mean(aug_stats[-50:])
                    method_info.append(f"Aug={avg_aug:.2f}")
                
                if method_info:
                    print(f"Methods: {', '.join(method_info)}")
                print()  # ë¹ˆ ì¤„

            # ì •ê¸°ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_interval = self.config.get('memory_cleanup_interval', 5)
            if step % cleanup_interval == 0:
                self.memory_manager.cleanup_memory(aggressive=True)

        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        avg_losses = {k: v/max(step, 1) for k, v in total_losses.items()}
        avg_losses['separation_loss'] = (avg_losses['s1_separation'] + avg_losses['s2_separation']) / 2
        
        if classification_accuracies:
            avg_losses['classification_accuracy'] = np.mean(classification_accuracies)

        return avg_losses
        
    def _validate(self):
        self.model.eval()

        total_losses = {
            'total': 0, 'anc_dba': 0, 'anc_nmse': 0, 'anc_total': 0,
            'final_quality': 0, 's1_separation': 0, 's2_separation': 0,
            'antinoise_constraint': 0, 'classification': 0
        }

        additional_metrics = {
            'processing_time': [],
            'classification_accuracy': []
        }

        num_batches = len(self.val_loader)

        val_bar = tqdm(self.val_loader, desc='Validation', ncols=100)

        for batch_idx, batch in enumerate(val_bar):
            try:
                start_time = time.time()

                # ë°ì´í„° ì „ì²˜ë¦¬
                with torch.no_grad():
                    if 'sources' in batch:
                        # Training íƒ€ì… (ì‹¤ì œë¡œëŠ” validationì—ì„œ ì•ˆ ë‚˜íƒ€ë‚¨)
                        sources = batch['sources'].to(self.device)
                        mixed_input = sources.sum(dim=1, keepdim=True)
                        
                        # ì •ê·œí™”
                        m = mixed_input.mean(dim=-1, keepdim=True)
                        s = mixed_input.std(dim=-1, keepdim=True)
                        mixed_input = (mixed_input - m) / (s + 1e-9)
                    else:
                        # Validation íƒ€ì… (premixed)
                        mixed_input = batch['input'].to(self.device)

                    adaptive_chunk = self.memory_manager.get_safe_chunk_size()

                    # ëª¨ë¸ forward
                    outputs = self.model.forward_for_training(
                        mixed_input, 
                        chunk_len=adaptive_chunk, 
                        return_classification=True
                    )

                # ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•´ í•„ìš”í•œ í…ì„œë“¤ì„ gradient í™œì„±í™”
                key_tensors = ['s1_clean', 's2_noise', 's2_enhanced', 'enhanced_verification']
                for key in key_tensors:
                    if key in outputs and outputs[key] is not None:
                        outputs[key] = outputs[key].detach().requires_grad_(True)
                
                # ì†ì‹¤ ê³„ì‚°
                losses = self._compute_losses(outputs, batch)

                # ê¸°ë³¸ ì†ì‹¤ë“¤ ëˆ„ì 
                for key in total_losses.keys():
                    loss_key = key + '_loss' if key != 'total' else 'total_loss'
                    if key == 'anc_total':
                        loss_key = 'anc_total_loss'
                    elif key == 'classification':
                        loss_key = 'classification_loss'

                    if loss_key in losses:
                        total_losses[key] += losses[loss_key].item()
                
                # ë¶„ë¥˜ ì •í™•ë„ ìˆ˜ì§‘
                if 'classification_accuracy' in losses:
                    additional_metrics['classification_accuracy'].append(losses['classification_accuracy'])

                # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
                processing_time = time.time() - start_time
                additional_metrics['processing_time'].append(processing_time)

                avg_total = total_losses['total'] / (batch_idx + 1)
                avg_anc = total_losses['anc_total'] / (batch_idx + 1)
                
                postfix = {
                    'Loss': f"{avg_total:.4f}",
                    'ANC': f"{avg_anc:.2f}dB"
                }
                
                if additional_metrics['classification_accuracy']:
                    recent_acc = np.mean(additional_metrics['classification_accuracy'])
                    postfix['Acc'] = f"{recent_acc:.2f}"
                
                val_bar.set_postfix(postfix)

            except Exception as e:
                print(f"Validation error: {e}")
                continue

        # í‰ê·  ê³„ì‚°
        avg_losses = {k: v/max(num_batches, 1) for k, v in total_losses.items()}
        avg_metrics = {k: np.mean(v) if v else 0.0 for k, v in additional_metrics.items()}

        avg_losses['separation_loss'] = (avg_losses['s1_separation'] + avg_losses['s2_separation']) / 2

        # ê²°í•©í•˜ì—¬ ë°˜í™˜
        result = {**avg_losses, **avg_metrics}

        return result
    
    def _log_epoch_summary(self, epoch, train_metrics, val_metrics):
        """ì—í¬í¬ ì™„ë£Œ í›„ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"EPOCH {epoch} SUMMARY")
        print(f"{'='*60}")
        
        # ğŸ¯ í•µì‹¬ ì§€í‘œ ë¹„êµ
        print(f"{'Metric':<20} {'Train':<12} {'Validation':<12} {'Status'}")
        print(f"{'-'*60}")
        
        # Total Loss
        train_total = train_metrics['total']
        val_total = val_metrics['total']
        status = "âœ…" if val_total < train_total * 1.2 else "âš ï¸"
        print(f"{'Total Loss':<20} {train_total:<12.4f} {val_total:<12.4f} {status}")
        
        # ANC
        train_anc = train_metrics['anc_total']
        val_anc = val_metrics['anc_total']
        status = "âœ…" if val_anc < train_anc + 1.0 else "âš ï¸"
        print(f"{'ANC (dB)':<20} {train_anc:<12.2f} {val_anc:<12.2f} {status}")
        
        # Separation
        train_sep = train_metrics.get('separation_loss', 0)
        val_sep = val_metrics.get('separation_loss', 0)
        status = "âœ…" if val_sep < train_sep * 1.3 else "âš ï¸"
        print(f"{'Separation':<20} {train_sep:<12.4f} {val_sep:<12.4f} {status}")
        
        # Classification
        if 'classification_accuracy' in val_metrics:
            train_acc = train_metrics.get('classification_accuracy', 0)
            val_acc = val_metrics['classification_accuracy']
            status = "âœ…" if val_acc > 0.6 else "âš ï¸"
            print(f"{'Classification':<20} {train_acc:<12.1%} {val_acc:<12.1%} {status}")
        
        print(f"{'='*60}\n")

    def calculate_composite_score(self, val_metrics):
        
        # ê° ë©”íŠ¸ë¦­ì„ 0-100 ì ìˆ˜ë¡œ ì •ê·œí™”
        def normalize_metric(value, target, direction='lower'):
            """
            direction: 'lower' = ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, 'higher' = ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            target: ëª©í‘œê°’ (100ì  ê¸°ì¤€)
            """
            if direction == 'lower':
                # ëª©í‘œê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ 100ì , ë†’ì„ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ
                if value <= target:
                    return 100.0
                else:
                    return max(0, 100 - (value - target) * 20)  # ì´ˆê³¼ì‹œ ë¹ ë¥´ê²Œ ê°ì 
            else:
                # ëª©í‘œê°’ë³´ë‹¤ ë†’ìœ¼ë©´ 100ì , ë‚®ì„ìˆ˜ë¡ ì ìˆ˜ ê°ì†Œ  
                if value >= target:
                    return 100.0
                else:
                    return max(0, value / target * 100)
        
        # ê° ì„±ëŠ¥ ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°
        anc_score = normalize_metric(
            val_metrics['anc_total'],  # abs() ì œê±°
            target=-12.0,  # ìŒìˆ˜ ëª©í‘œê°’
            direction='lower'  # ë” ë‚®ì€(ë” ìŒìˆ˜) ê°’ì´ ì¢‹ìŒ
        )
        
        separation_score = normalize_metric(
            val_metrics.get('separation_loss', val_metrics.get('s1_separation', 999)), 
            target=1.5,   # SI-SDR 1.5 ëª©í‘œ
            direction='lower'
        )
        
        classification_score = normalize_metric(
            val_metrics.get('classification_accuracy', 0) * 100,
            target=80.0,  # 80% ì •í™•ë„ ëª©í‘œ
            direction='higher'
        )
        
        final_quality_score = normalize_metric(
            val_metrics['final_quality'],
            target=1.0,   # ìµœì¢… í’ˆì§ˆ ì†ì‹¤ 1.0 ëª©í‘œ
            direction='lower'
        )
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        weights = {
            'anc': 0.35,           # ANC ì„±ëŠ¥ 35%
            'separation': 0.30,    # ë¶„ë¦¬ ì„±ëŠ¥ 30%
            'classification': 0.25, # ë¶„ë¥˜ ì„±ëŠ¥ 25%
            'final_quality': 0.10  # ìµœì¢… í’ˆì§ˆ 10%
        }
        
        composite_score = (
            anc_score * weights['anc'] +
            separation_score * weights['separation'] + 
            classification_score * weights['classification'] +
            final_quality_score * weights['final_quality']
        )
        
        return {
            'composite_score': composite_score,
            'anc_score': anc_score,
            'separation_score': separation_score, 
            'classification_score': classification_score,
            'final_quality_score': final_quality_score
        }
        
    def _save_checkpoint(self, epoch, metrics, is_best=False):
        state = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'metrics': metrics,
            'config': self.config,
            'memory_stats': self.memory_manager.get_memory_usage(),
            'use_online_augment': self.use_online_augment,
            'use_pit_loss': self.use_pit_loss
        }

        # í•­ìƒ ìµœì‹  ëª¨ë¸ ì €ì¥
        latest_path = os.path.join(self.checkpoint_path, 'latest_checkpoint.pth')
        torch.save(state, latest_path)

        # ì¢…í•© ë² ìŠ¤íŠ¸ ëª¨ë¸
        scores = self.calculate_composite_score(metrics)
        if scores['composite_score'] > getattr(self, 'best_composite_score', 0):
            self.best_composite_score = scores['composite_score']
            best_composite_path = os.path.join(self.checkpoint_path, 'best_composite.pth')
            torch.save(state, best_composite_path)
            print(f"NEW BEST COMPOSITE MODEL! Score: {scores['composite_score']:.1f}")
        
        # ê°œë³„ ì„±ëŠ¥ ë² ìŠ¤íŠ¸ ëª¨ë¸ë“¤
        
        # ANC ë² ìŠ¤íŠ¸
        if metrics['anc_total'] < getattr(self, 'best_anc_performance', 0):  # ë” ìŒìˆ˜ê°€ ì¢‹ìŒ
            self.best_anc_performance = metrics['anc_total']
            best_anc_path = os.path.join(self.checkpoint_path, 'best_anc.pth')
            torch.save(state, best_anc_path)
            print(f"NEW BEST ANC MODEL! ANC: {metrics['anc_total']:.2f}dB")
        
        # ë¶„ë¦¬ ì„±ëŠ¥ ë² ìŠ¤íŠ¸
        sep_loss = metrics.get('separation_loss', metrics.get('s1_separation', 999))
        if sep_loss < getattr(self, 'best_separation_loss', float('inf')):
            self.best_separation_loss = sep_loss
            best_sep_path = os.path.join(self.checkpoint_path, 'best_separation.pth')
            torch.save(state, best_sep_path)
            print(f"NEW BEST SEPARATION MODEL! Sep: {sep_loss:.3f}")
        
        # ë¶„ë¥˜ ì„±ëŠ¥ ë² ìŠ¤íŠ¸
        cls_acc = metrics.get('classification_accuracy', 0)
        if cls_acc > getattr(self, 'best_classification_accuracy', 0):
            self.best_classification_accuracy = cls_acc
            best_cls_path = os.path.join(self.checkpoint_path, 'best_classification.pth')
            torch.save(state, best_cls_path)
            print(f"NEW BEST CLASSIFICATION MODEL! Acc: {cls_acc:.1%}")

    def _log_metrics(self, epoch, train_metrics, val_metrics):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if hasattr(self, 'writer'):
            # ê¸°ë³¸ ì†ì‹¤ë“¤
            for loss_name in ['total', 'anc_total', 'final_quality', 'classification']:
                self.writer.add_scalars(f'Loss/{loss_name}', {
                    'Train': train_metrics[loss_name],
                    'Val': val_metrics[loss_name]
                }, epoch)

            # PIT ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
            if self.use_pit_loss:
                self.writer.add_scalar('Training/PIT_Loss_Active', 1, epoch)
            else:
                self.writer.add_scalar('Training/PIT_Loss_Active', 0, epoch)

            # ì¶”ê°€ ë©”íŠ¸ë¦­ë“¤
            if 'classification_accuracy' in val_metrics:
                self.writer.add_scalar('Metrics/Classification_Accuracy',
                                    val_metrics['classification_accuracy'], epoch)

            if 'processing_time' in val_metrics:
                self.writer.add_scalar('Metrics/Processing_Time',
                                     val_metrics['processing_time'], epoch)

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            allocated, reserved = self.memory_manager.get_memory_usage()
            self.writer.add_scalar('System/Memory_Allocated', allocated, epoch)
            self.writer.add_scalar('System/Memory_Reserved', reserved, epoch)

    def train(self):
        epochs = self.config.get('epochs', 15)
    
        # ë² ìŠ¤íŠ¸ ì„±ëŠ¥ ì´ˆê¸°í™”
        self.best_composite_score = 0
        self.best_anc_performance = 0
        self.best_separation_loss = float('inf')
        self.best_classification_accuracy = 0

        print(f"Starting Mixed Training with Multi-Criteria Best Model Selection")
        print(f"Epochs: {epochs}")
        print(f"Training: Dynamic Mixing (spk1 + spk2)")
        print(f"Validation: Pre-mixed (mixtures, spk1, spk2)")
        print(f"PIT Loss: {self.use_pit_loss}")
        print(f"Online Augmentation: {self.use_online_augment}")
        
        # ğŸ”§ Early Stopping ì„¤ì • ì •ë³´ ì¶œë ¥ (í•œ ë²ˆë§Œ)
        print(f"Early Stopping Configuration:")
        print(f"Method: Multi-Metric (ANC: {self.early_stopping.metric_weights['anc_total']:.1%}, "
            f"Sep: {self.early_stopping.metric_weights['separation_loss']:.1%}, "
            f"Cls: {self.early_stopping.metric_weights['classification_accuracy']:.1%}, "
            f"Qual: {self.early_stopping.metric_weights['final_quality']:.1%})")
        print(f"   Patience: {self.early_stopping.patience}, Min Delta: {self.early_stopping.min_delta}")
        
        for epoch in range(1, epochs + 1):
            print(f"\n EPOCH {epoch}/{epochs}")

            try:
                # í›ˆë ¨
                train_metrics = self._train_epoch(epoch)

                # ê²€ì¦
                if self.val_loader:
                    val_metrics = self._validate()
                else:
                    val_metrics = train_metrics.copy()

                # ì—í¬í¬ ìš”ì•½ ì¶œë ¥
                self._log_epoch_summary(epoch, train_metrics, val_metrics)
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚° (íŠ¸ë ˆì´ë„ˆì˜ ê¸°ì¡´ í•¨ìˆ˜)
                scores = self.calculate_composite_score(val_metrics)
                
                # Early Stopping ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ë°©ì‹)
                self.early_stopping.update(val_metrics)

                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                composite_score_for_scheduler = (
                    val_metrics['anc_total'] * 0.4 +           # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ìŒìˆ˜ê°’)
                    val_metrics['separation_loss'] * 0.3 +     # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                    -val_metrics.get('classification_accuracy', 0) * 0.2 + # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ìŒìˆ˜ ë³€í™˜)
                    val_metrics['final_quality'] * 0.1         # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                )
                self.scheduler.step(composite_score_for_scheduler)
                
                # ì¢…í•© ì ìˆ˜ í‘œì‹œ (íŠ¸ë ˆì´ë„ˆì˜ ê¸°ì¡´ ì ìˆ˜ + Early Stopping ì ìˆ˜)
                print(f"Trainer Composite Score: {scores['composite_score']:.1f} "
                    f"(ANC:{scores['anc_score']:.0f}, Sep:{scores['separation_score']:.0f}, "
                    f"Cls:{scores['classification_score']:.0f}, Qual:{scores['final_quality_score']:.0f})")

                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"LR: {current_lr:.2e}")

                # TensorBoard ë¡œê¹…
                self._log_metrics(epoch, train_metrics, val_metrics)

                # ë‹¤ì¤‘ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (íŠ¸ë ˆì´ë„ˆì˜ ê¸°ì¡´ í•¨ìˆ˜)
                self._save_checkpoint(epoch, val_metrics)

                # ê°œì„  ìƒí™© ìš”ì•½ (2 ì—í¬í¬ë§ˆë‹¤, Early Stoppingì—ì„œ)
                if epoch % 2 == 0:
                    improvement_summary = self.early_stopping.get_improvement_summary()
                    print(f"Improvement Summary: {improvement_summary}")

                # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
                if self.early_stopping.should_stop():
                    best_scores = self.early_stopping.get_best_scores()
                    print(f"\n Early stopping after {epoch} epochs")
                    print(f"Early Stopping Best Composite Score: {best_scores['composite_score']:.4f}")
                    print(f"Best Individual Scores: {best_scores['individual_bests']}")
                    break

            except Exception as e:
                print(f"Error in epoch {epoch}: {e}")
                self.memory_manager.cleanup_memory(aggressive=True)
                continue

            # ì—í¬í¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_manager.cleanup_memory(aggressive=True)

        print(f"\n MIXED TRAINING COMPLETED!")
        print(f" Best Composite Score: {self.best_composite_score:.1f}")
        print(f" Best ANC: {self.best_anc_performance:.2f}dB")
        print(f" Best Separation: {self.best_separation_loss:.3f}")
        print(f" Best Classification: {self.best_classification_accuracy:.1%}")
        print(f" Results saved in: {self.exp_path}")
        print(f"\n Available models:")
        print(f"   - best_composite.pth (ì¢…í•© ìµœê³ )")
        print(f"   - best_anc.pth (ANC ìµœê³ )")  
        print(f"   - best_separation.pth (ë¶„ë¦¬ ìµœê³ )")
        print(f"   - best_classification.pth (ë¶„ë¥˜ ìµœê³ )")