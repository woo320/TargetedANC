import torch
import gc
import psutil
from config.constants import SR

class MemoryManager:

    def __init__(self, config):
        self.max_memory_gb = config.get('max_memory_gb', 22.5)
        self.memory_warning_threshold = config.get('memory_warning_threshold', 20.0)
        self.memory_critical_threshold = config.get('memory_critical_threshold', 21.5)
        self.adaptive_chunk_sizes = config.get('adaptive_chunk_sizes', {
            'low_memory': 1.0 * 16000,
            'medium_memory': 1.5 * 16000,
            'normal_memory': 2.0 * 16000,
            'high_memory': 3.0 * 16000
        })
        self.memory_history = []

    def get_memory_usage(self):
        """í˜„ìž¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (GB)"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            return allocated, reserved
        return 0, 0
    
    def get_safe_chunk_size(self):
        """í˜„ìž¬ ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ì•ˆì „í•œ ì²­í¬ í¬ê¸° ë°˜í™˜"""
        allocated, reserved = self.get_memory_usage()

        if allocated > self.memory_critical_threshold:
            return int(self.adaptive_chunk_sizes['low_memory'])
        elif allocated > self.memory_warning_threshold:
            return int(self.adaptive_chunk_sizes['medium_memory'])
        elif allocated > 15:
            return int(self.adaptive_chunk_sizes['normal_memory'])
        else:
            return int(self.adaptive_chunk_sizes['high_memory'])

    def is_memory_critical(self):
        """ë©”ëª¨ë¦¬ê°€ ìœ„í—˜ ìˆ˜ì¤€ì¸ì§€ í™•ì¸"""
        allocated, _ = self.get_memory_usage()
        return allocated > self.memory_critical_threshold

    def get_system_memory(self):
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        return psutil.virtual_memory().used / 1e9

    def cleanup_memory(self, aggressive=False):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()

        if aggressive:
            for _ in range(3):
                gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def adaptive_batch_size(self, default_batch_size=1):
        allocated, _ = self.get_memory_usage()
        
        if allocated > self.memory_critical_threshold:   
            return 1
        elif allocated > self.memory_warning_threshold:  
            # ðŸ”§ ì•ˆì „í•œ ì ˆë°˜ ê³„ì‚°
            if default_batch_size <= 1:
                return 1
            else:
                return max(1, default_batch_size // 2)
        elif allocated < 10.0:  # ë©”ëª¨ë¦¬ ì—¬ìœ  (10GB ë¯¸ë§Œ ì‚¬ìš©)
            # ðŸ”§ íš¨ê³¼ì ì¸ ë°°ì¹˜ ì¦ê°€
            return min(max(default_batch_size * 2, 4), 8)
        else:
            return default_batch_size